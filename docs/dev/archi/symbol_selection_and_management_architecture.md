# Symbol Selection and Management Architecture

**Document ID**: ARCH-SYMBOL-MGMT-001  
**Version**: 1.2.0  
**Date**: 2026-01-20  
**Status**: Production Optimized  
**Author**: Senior Architect & Developer (20 years)  
**Parent Documents**: 
- [[data_pipeline_architecture_observer_v1.0.md]]
- [[obs_architecture.md]]
- [[kis_api_specification_v1.0.md]]
- [[observer_architecture_v2.md]]

**Version History**:
- v1.0.0 (2026-01-20): Initial document creation
- v1.1.0 (2026-01-20): Enhanced with production-ready features
  - Data Gap Recovery mechanism
  - Resource Isolation strategy
  - Multi-Account Scalability design
  - Advanced Session Lifecycle management
- v1.2.0 (2026-01-20): Operational optimization
  - Differentiated operation timeline (Track A vs Track B)
  - Intentional noise filtering strategy (09:00-09:30, 15:00-15:30)
  - Context synchronization at Track B startup
  - OperationScheduler for time-based control

---

## Table of Contents

1. [Overview](#overview)
2. [Session and Token Management](#session-and-token-management)
3. [Universe Management](#universe-management)
4. [Slot Management](#slot-management)
5. [Trigger-based Selection](#trigger-based-selection)
6. [Data Continuity and Gap Recovery](#data-continuity-and-gap-recovery)
7. [Resource Isolation Architecture](#resource-isolation-architecture)
8. [Multi-Account Scalability](#multi-account-scalability)
9. [Capacity Planning](#capacity-planning)
10. [Implementation Guide](#implementation-guide)
11. [Constraints and Considerations](#constraints-and-considerations)
12. [Appendix](#appendix)

---

## Overview

### Purpose

Î≥∏ Î¨∏ÏÑúÎäî Stock Trading Observer ÏãúÏä§ÌÖúÏóêÏÑú **Ï¢ÖÎ™© ÏÑ†Ï†ï(Symbol Selection)** Î∞è **Ï¢ÖÎ™© Í¥ÄÎ¶¨(Symbol Management)** ÏïÑÌÇ§ÌÖçÏ≤òÎ•º Ï†ïÏùòÌï©ÎãàÎã§. Îã§Ïùå ÏÑ∏ Í∞ÄÏßÄ ÌïµÏã¨ ÏòÅÏó≠ÏùÑ Îã§Î£πÎãàÎã§:

1. **Universe Management**: Í±∞Îûò ÎåÄÏÉÅ Ï¢ÖÎ™© ÌíÄ(pool) ÏÉùÏÑ± Î∞è Í¥ÄÎ¶¨
2. **Slot Management**: WebSocket Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ Ïä¨Î°Ø(41Í∞ú) Í¥ÄÎ¶¨
3. **Trigger-based Selection**: Ïù¥Î≤§Ìä∏ Í∏∞Î∞ò Ï¢ÖÎ™© ÏÑ†Ï†ï Î∞è ÎèôÏ†Å ÍµêÏ≤¥

### Scope

**In Scope:**
- Ï¢ÖÎ™© ÏÑ†Ï†ï Í∏∞Ï§Ä Î∞è ÌîÑÎ°úÏÑ∏Ïä§
- ÏµúÎåÄ Ï¢ÖÎ™© Ïàò Ï†úÏïΩ Î∞è Í¥ÄÎ¶¨ Î∞©Ïãù
- Ïã§ÏãúÍ∞Ñ Ïù¥Î≤§Ìä∏ Î∞úÏÉù Ïãú Ï¢ÖÎ™© ÏÑ†Ï†ï Î°úÏßÅ
- API Rate Limit Í≥†Î†§Ìïú Ïö©Îüâ Í≥ÑÌöç
- Íµ¨ÌòÑ Í∞ÄÏù¥Îìú Î∞è Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Í≥Ñ

**Out of Scope:**
- Ìä∏Î†àÏù¥Îî© Ï†ÑÎûµ Î∞è ÏùòÏÇ¨Í≤∞Ï†ï Î°úÏßÅ
- Ï£ºÎ¨∏ Ïã§Ìñâ ÏãúÏä§ÌÖú
- Î∞±ÌÖåÏä§ÌåÖ Î∞è ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏóîÏßÑ
- Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù Î∞è ÏãúÍ∞ÅÌôî

### System Context

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    External APIs                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ KIS API  ‚îÇ  ‚îÇ  Kiwoom  ‚îÇ  ‚îÇ  Upbit   ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Provider Engine          ‚îÇ
        ‚îÇ   (API Abstraction)        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Universe Manager         ‚îÇ
        ‚îÇ   - Daily Snapshot         ‚îÇ
        ‚îÇ   - Symbol Pool (4000+)    ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ üìÑ THIS DOCUMENT
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Track A Collector        ‚îÇ
        ‚îÇ   - 10min Interval         ‚îÇ
        ‚îÇ   - Full Universe          ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Trigger Engine           ‚îÇ
        ‚îÇ   - Volume Surge           ‚îÇ
        ‚îÇ   - Trade Velocity         ‚îÇ
        ‚îÇ   - Volatility Spike       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Slot Manager             ‚îÇ
        ‚îÇ   - 41 Slots MAX           ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ üìÑ THIS DOCUMENT
        ‚îÇ   - Overflow Handling      ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Track B Collector        ‚îÇ
        ‚îÇ   - WebSocket Real-time    ‚îÇ
        ‚îÇ   - 2Hz (41 symbols max)   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Principles

1. **Coverage Maximization**: ÏµúÎåÄÌïú ÎßéÏùÄ Ï¢ÖÎ™©ÏùÑ Î™®ÎãàÌÑ∞ÎßÅ (Ï†úÏïΩ ÎÇ¥ÏóêÏÑú)
2. **Event-Driven Priority**: Ïù¥Î≤§Ìä∏ Î∞úÏÉù Ïãú Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò ÏÑ†Ï†ï
3. **API Efficiency**: Rate Limit Ï§ÄÏàòÌïòÎ©∞ ÏµúÏ†ÅÌôîÎêú API ÏÇ¨Ïö©
4. **Transparency**: ÏÑ†Ï†ï/Ï†úÏô∏ Í≥ºÏ†ï ÏôÑÏ†Ñ Ï∂îÏ†Å Í∞ÄÎä•
5. **Reproducibility**: ÎèôÏùº Ï°∞Í±¥ÏóêÏÑú ÎèôÏùº Í≤∞Í≥º Î≥¥Ïû•
6. **Data Continuity**: WebSocket Ïû¨Ïó∞Í≤∞ Ïãú Îç∞Ïù¥ÌÑ∞ Í≥µÎ∞± ÏûêÎèô Î≥¥Ï†ï **(v1.1 Ï∂îÍ∞Ä)**
7. **Resource Isolation**: Track A/B Î¨ºÎ¶¨Ï†Å Í≤©Î¶¨Î°ú ÏÉÅÌò∏ Í∞ÑÏÑ≠ Î∞©ÏßÄ **(v1.1 Ï∂îÍ∞Ä)**
8. **Scalability**: Î©ÄÌã∞ Í≥ÑÏ†ï ÏßÄÏõêÏùÑ ÌÜµÌïú Ïä¨Î°Ø ÌôïÏû•ÏÑ± ÌôïÎ≥¥ **(v1.1 Ï∂îÍ∞Ä)**

---

## Session and Token Management

### Token Lifecycle Overview

KIS APIÏùò OAuth 2.0 ÌÜ†ÌÅ∞ÏùÄ **24ÏãúÍ∞Ñ Ïú†Ìö®Í∏∞Í∞Ñ**ÏùÑ Í∞ÄÏßÄÎ©∞, Ïû•Ï§ë ÌÜ†ÌÅ∞ ÎßåÎ£åÎ°ú Ïù∏Ìïú ÏÑúÎπÑÏä§ Ï§ëÎã®ÏùÑ Î∞©ÏßÄÌïòÍ∏∞ ÏúÑÌï¥ Ï≤¥Í≥ÑÏ†ÅÏù∏ ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨ Ï†ïÏ±ÖÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Daily Token Lifecycle                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  05:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Universe Generation                        ‚îÇ
‚îÇ              (Use yesterday's token)                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  08:30 ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Pre-Market Token Refresh ‚ú® (NEW)        ‚îÇ
‚îÇ              - Force token renewal                       ‚îÇ
‚îÇ              - WebSocket session restart                 ‚îÇ
‚îÇ              - Health check                              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  09:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Track A Start (REST)                       ‚îÇ
‚îÇ  09:30 ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Track B Start (WebSocket)                  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  15:30 ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Track A Stop                               ‚îÇ
‚îÇ  15:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Track B Stop                               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  21:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Daily Backup                               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  23:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Proactive Token Refresh                    ‚îÇ
‚îÇ              (If token age > 23h)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### System Wake-up and Pre-Market Preparation (v1.2)

**Î™©Ï†Å**: Ïû• ÏãúÏûë 1ÏãúÍ∞Ñ Ï†Ñ ÏãúÏä§ÌÖú ÏòàÏó¥ Î∞è ÌÜ†ÌÅ∞ Í∞±Ïã†

#### Wake-up Schedule

| ÏãúÍ∞Ñ | ÏûëÏóÖ | Ïö∞ÏÑ†ÏàúÏúÑ | ÏÑ§Î™Ö |
|-----|------|---------|------|
| **08:00** | System Wake-up | CRITICAL | ÏãúÏä§ÌÖú Í∏∞Îèô Î∞è Ï¥àÍ∏∞Ìôî |
| **08:01** | Token Renewal | CRITICAL | Ïû• ÏãúÏûë 90Î∂Ñ Ï†Ñ Í∞ïÏ†ú Í∞±Ïã† |
| **08:03** | WebSocket Pre-establish | HIGH | WS Ïó∞Í≤∞ ÏÇ¨Ï†Ñ Íµ¨Ï∂ï (Íµ¨ÎèÖ ÏóÜÏùå) |
| **08:05** | System Warmup | MEDIUM | Ï∫êÏãú ÏòàÏó¥, DB Ïó∞Í≤∞ ÌôïÏù∏ |
| **08:10** | Health Check | HIGH | Î™®Îì† ÏãúÏä§ÌÖú Ïª¥Ìè¨ÎÑåÌä∏ Ï†êÍ≤Ä |
| **08:50** | Ready Signal | LOW | Ïû• ÏãúÏûë 10Î∂Ñ Ï†Ñ Ï§ÄÎπÑ ÏôÑÎ£å |

#### Implementation

```python
import asyncio
from datetime import datetime, time, timezone
from typing import Optional
import logging

logger = logging.getLogger("TokenManager")


class EnhancedTokenManager:
    """
    Í≥†Í∏â ÌÜ†ÌÅ∞ Í¥ÄÎ¶¨Ïûê (v1.1)
    
    Features:
    - Pre-market token refresh (08:30)
    - Proactive renewal (23h threshold)
    - WebSocket session coordination
    - Failure recovery mechanism
    """
    
    def __init__(self, kis_client, websocket_manager):
        self.kis_client = kis_client
        self.ws_manager = websocket_manager
        self.token_issued_at: Optional[datetime] = None
        self.token_expires_at: Optional[datetime] = None
        self.refresh_lock = asyncio.Lock()
        
    async def start_lifecycle_manager(self):
        """
        ÌÜ†ÌÅ∞ ÎùºÏù¥ÌîÑÏÇ¨Ïù¥ÌÅ¥ Í¥ÄÎ¶¨ Îç∞Î™¨ ÏãúÏûë
        """
        while True:
            try:
                now = datetime.now(timezone.utc)
                
                # Pre-market refresh check (08:30 KST = 23:30 UTC-1)
                if self._is_pre_market_time(now):
                    await self._execute_pre_market_refresh()
                    await asyncio.sleep(3600)  # 1ÏãúÍ∞Ñ ÎåÄÍ∏∞ (Ï§ëÎ≥µ Î∞©ÏßÄ)
                    continue
                
                # Proactive refresh check (23h threshold)
                if self._should_proactive_refresh():
                    await self._execute_proactive_refresh()
                
                # 1Î∂ÑÎßàÎã§ Ï≤¥ÌÅ¨
                await asyncio.sleep(60)
                
            except Exception as e:
                logger.error(f"Token lifecycle error: {e}", exc_info=True)
                await asyncio.sleep(60)
    
    def _is_pre_market_time(self, dt: datetime) -> bool:
        """
        08:30 KST ÏãúÍ∞ÑÎåÄÏù∏ÏßÄ ÌôïÏù∏
        """
        # Convert to KST (UTC+9)
        kst_time = dt.astimezone(timezone(timedelta(hours=9)))
        target_time = time(8, 30)
        
        # 08:30 ~ 08:35 ÏÇ¨Ïù¥
        return (kst_time.time() >= target_time and 
                kst_time.time() < time(8, 35))
    
    def _should_proactive_refresh(self) -> bool:
        """
        ÌÜ†ÌÅ∞ ÎÇòÏù¥Í∞Ä 23ÏãúÍ∞Ñ Ïù¥ÏÉÅÏù∏ÏßÄ ÌôïÏù∏
        """
        if not self.token_issued_at:
            return False
        
        age = datetime.now(timezone.utc) - self.token_issued_at
        return age.total_seconds() >= (23 * 3600)
    
    async def _execute_pre_market_refresh(self):
        """
        Pre-market token refresh Ïã§Ìñâ
        """
        async with self.refresh_lock:
            logger.info("=" * 60)
            logger.info("PRE-MARKET TOKEN REFRESH INITIATED")
            logger.info("Time: 08:30 KST (30 min before market open)")
            logger.info("=" * 60)
            
            try:
                # Step 1: WebSocket graceful shutdown
                logger.info("[1/5] Shutting down WebSocket connections...")
                await self.ws_manager.graceful_shutdown(
                    reason="pre_market_refresh"
                )
                
                # Step 2: Request new token
                logger.info("[2/5] Requesting new OAuth token...")
                new_token = await self.kis_client.refresh_token()
                self.token_issued_at = datetime.now(timezone.utc)
                self.token_expires_at = self.token_issued_at + timedelta(hours=24)
                
                logger.info(f"[2/5] New token expires at: {self.token_expires_at}")
                
                # Step 3: Update WebSocket approval key
                logger.info("[3/5] Updating WebSocket approval key...")
                new_approval_key = await self.kis_client.get_approval_key()
                self.ws_manager.update_approval_key(new_approval_key)
                
                # Step 4: Restart WebSocket connections
                logger.info("[4/5] Restarting WebSocket connections...")
                await self.ws_manager.reconnect_all()
                
                # Step 5: Health check
                logger.info("[5/5] Running system health check...")
                health = await self._run_health_check()
                
                if health['status'] == 'healthy':
                    logger.info("‚úÖ PRE-MARKET REFRESH COMPLETED SUCCESSFULLY")
                    logger.info(f"System ready for market open at 09:00")
                else:
                    logger.error(f"‚ö†Ô∏è HEALTH CHECK FAILED: {health}")
                    await self._send_alert(
                        "Pre-market refresh health check failed",
                        health
                    )
                
            except Exception as e:
                logger.critical(
                    f"‚ùå PRE-MARKET REFRESH FAILED: {e}",
                    exc_info=True
                )
                await self._send_alert(
                    "CRITICAL: Pre-market token refresh failed",
                    {"error": str(e)}
                )
                raise
    
    async def _execute_proactive_refresh(self):
        """
        Proactive token refresh (23h threshold)
        """
        async with self.refresh_lock:
            logger.info("Proactive token refresh (23h threshold)")
            
            try:
                new_token = await self.kis_client.refresh_token()
                self.token_issued_at = datetime.now(timezone.utc)
                self.token_expires_at = self.token_issued_at + timedelta(hours=24)
                
                logger.info(f"Token refreshed, expires at: {self.token_expires_at}")
                
            except Exception as e:
                logger.error(f"Proactive refresh failed: {e}")
                # Ïã§Ìå®Ìï¥ÎèÑ Í∏∞Ï°¥ ÌÜ†ÌÅ∞Ïù¥ ÏïÑÏßÅ Ïú†Ìö®ÌïòÎØÄÎ°ú Í≥ÑÏÜç ÏßÑÌñâ
    
    async def _run_health_check(self) -> dict:
        """
        ÏãúÏä§ÌÖú Ìó¨Ïä§ Ï≤¥ÌÅ¨
        """
        checks = {
            'token_valid': await self._check_token_validity(),
            'websocket_connected': self.ws_manager.is_connected(),
            'rest_api_accessible': await self._check_rest_api(),
            'slot_manager_ready': True  # SlotManager check
        }
        
        status = 'healthy' if all(checks.values()) else 'degraded'
        
        return {
            'status': status,
            'checks': checks,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
    
    async def _check_token_validity(self) -> bool:
        """ÌÜ†ÌÅ∞ Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù"""
        try:
            # Simple API call to verify token
            response = await self.kis_client.get_balance()
            return response.status_code == 200
        except:
            return False
    
    async def _check_rest_api(self) -> bool:
        """REST API Ï†ëÍ∑ºÏÑ± ÌôïÏù∏"""
        try:
            response = await self.kis_client.fetch_price("005930")
            return response is not None
        except:
            return False
    
    async def _send_alert(self, subject: str, details: dict):
        """ÏïåÎ¶º Î∞úÏÜ° (Î©îÏùº, Slack Îì±)"""
        logger.critical(f"ALERT: {subject}")
        logger.critical(f"Details: {details}")
        # TODO: Implement actual alert mechanism
```

### Token Refresh Policy Matrix

| ÏãúÎÇòÎ¶¨Ïò§ | Ï°∞Í±¥ | Ïï°ÏÖò | Ïö∞ÏÑ†ÏàúÏúÑ |
|---------|------|------|----------|
| **Pre-Market Refresh** | Îß§Ïùº 08:30 KST | Í∞ïÏ†ú Í∞±Ïã† + WS Ïû¨ÏãúÏûë | CRITICAL |
| **Proactive Refresh** | ÌÜ†ÌÅ∞ ÎÇòÏù¥ ‚â• 23h | ÏûêÎèô Í∞±Ïã† (WS Ïú†ÏßÄ) | HIGH |
| **Emergency Refresh** | 401 Unauthorized | Ï¶âÏãú Í∞±Ïã† + Ïû¨ÏãúÎèÑ | CRITICAL |
| **Scheduled Refresh** | Îß§Ïùº 23:00 | Í∞±Ïã† ÏãúÎèÑ (ÏòµÏÖò) | MEDIUM |

### WebSocket Session Coordination

**Challenge**: ÌÜ†ÌÅ∞ Í∞±Ïã† Ïãú WebSocket Ïó∞Í≤∞ÎèÑ ÏÉà Approval KeyÎ°ú Ïû¨ÏãúÏûë ÌïÑÏöî

**Solution**: Graceful Shutdown + Slot Preservation

```python
class WebSocketSessionManager:
    """
    WebSocket ÏÑ∏ÏÖò Í¥ÄÎ¶¨ with Token Coordination
    """
    
    async def graceful_shutdown(self, reason: str):
        """
        Ïö∞ÏïÑÌïú Ï¢ÖÎ£å (Ïä¨Î°Ø ÏÉÅÌÉú Î≥¥Ï°¥)
        """
        logger.info(f"WebSocket graceful shutdown: {reason}")
        
        # 1. ÌòÑÏû¨ ÌôúÏÑ± Ïä¨Î°Ø Ï†ÄÏû•
        active_slots = self.slot_manager.get_active_slots()
        self.preserved_slots = [
            {
                'slot_id': slot.slot_id,
                'symbol': slot.symbol,
                'candidate': slot.candidate,
                'priority_score': slot.candidate.priority_score
            }
            for slot in active_slots
        ]
        
        logger.info(f"Preserved {len(self.preserved_slots)} active slots")
        
        # 2. Î™®Îì† Ï¢ÖÎ™© unsubscribe
        for slot in active_slots:
            await self._unsubscribe(slot.symbol)
        
        # 3. WebSocket Ïó∞Í≤∞ Ï¢ÖÎ£å
        await self.websocket.close()
        
        logger.info("WebSocket shutdown complete")
    
    async def reconnect_all(self):
        """
        Ïû¨Ïó∞Í≤∞ Î∞è Ïä¨Î°Ø Î≥µÏõê
        """
        logger.info("WebSocket reconnecting...")
        
        # 1. ÏÉà WebSocket Ïó∞Í≤∞
        await self.websocket.connect()
        
        # 2. Î≥¥Ï°¥Îêú Ïä¨Î°Ø Î≥µÏõê (Ïö∞ÏÑ†ÏàúÏúÑ Ïàú)
        sorted_slots = sorted(
            self.preserved_slots,
            key=lambda s: s['priority_score'],
            reverse=True
        )
        
        for slot_data in sorted_slots[:41]:  # Max 41 slots
            await self._subscribe(slot_data['symbol'])
            logger.info(f"Restored slot: {slot_data['symbol']}")
        
        logger.info(f"Reconnected with {len(sorted_slots[:41])} slots")
        
        # 3. Ïä¨Î°Ø ÏÉÅÌÉú ÎèôÍ∏∞Ìôî
        await self.slot_manager.sync_from_preserved(self.preserved_slots)
```

### Failure Recovery Scenarios

#### Scenario 1: Pre-Market Refresh Fails

**Impact**: Ïû•Ï§ë ÌÜ†ÌÅ∞ ÎßåÎ£å Í∞ÄÎä•ÏÑ±

**Recovery**:
1. Ï¶âÏãú Ïû¨ÏãúÎèÑ (ÏµúÎåÄ 3Ìöå)
2. Ïã§Ìå® Ïãú Í∏∞Ï°¥ ÌÜ†ÌÅ∞ÏúºÎ°ú Í≥ÑÏÜç Ïö¥ÏòÅ
3. Proactive refresh ÏùòÏ°¥ (23h threshold)
4. CRITICAL ÏïåÎ¶º Î∞úÏÜ°

#### Scenario 2: WebSocket Reconnect Fails

**Impact**: Track B Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Ï§ëÎã®

**Recovery**:
1. Exponential backoff Ïû¨Ïó∞Í≤∞ (1s, 2s, 5s, 10s, 30s, 60s)
2. 60Ï¥à ÌõÑÏóêÎèÑ Ïã§Ìå® Ïãú Track B ÏùºÏãú Ï§ëÎã®
3. Track AÎäî Í≥ÑÏÜç Ïö¥ÏòÅ (ÎèÖÎ¶ΩÏ†Å)
4. Gap marker Í∏∞Î°ù

#### Scenario 3: Token Expired During Trading

**Impact**: Î™®Îì† API Ìò∏Ï∂ú Ïã§Ìå®

**Recovery**:
1. 401 ÏóêÎü¨ Í∞êÏßÄ Ï¶âÏãú Emergency Refresh Î∞úÎèô
2. ÏÉà ÌÜ†ÌÅ∞ Î∞úÍ∏â (ÏïΩ 1Ï¥à)
3. Ïã§Ìå®Ìïú ÏöîÏ≤≠ Ïû¨ÏãúÎèÑ
4. WebSocket Ïû¨Ïó∞Í≤∞ (ÏÉà Approval Key)
5. Í≥µÎ∞± Í∏∞Í∞Ñ Gap Recovery Ïã§Ìñâ (Îã§Ïùå ÏÑπÏÖò Ï∞∏Ï°∞)

---

## Universe Management

### Concept

**Universe**Îäî Í±∞Îûò Í∞ÄÎä•Ìïú Ï¢ÖÎ™©Ïùò Ï†ÑÏ≤¥ ÏßëÌï©(pool)ÏûÖÎãàÎã§. ObserverÎäî Ïù¥ UniverseÏóêÏÑú Ïã§Ï†ú Î™®ÎãàÌÑ∞ÎßÅÌï† Ï¢ÖÎ™©ÏùÑ ÏÑ†Ï†ïÌï©ÎãàÎã§.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              KRX All Stocks (~2,500)                ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   Universe (Close >= 4,000 KRW)           ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   Expected: 800 ~ 1,200 symbols           ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                            ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Track A (Full Coverage)          ‚îÇ    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  All Universe symbols             ‚îÇ    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  10min interval                   ‚îÇ    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                            ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Track B (41 Slots)               ‚îÇ    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Trigger-based selection          ‚îÇ    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Real-time WebSocket              ‚îÇ    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Universe Generation Process

#### Daily Snapshot Creation

UniverseÎäî **ÏùºÏùº Îã®ÏúÑ**Î°ú ÏÉùÏÑ±ÎêòÎ©∞, ÎãπÏùº Ï§ëÏóêÎäî Í≥†Ï†ïÎê©ÎãàÎã§.

```mermaid
sequenceDiagram
    participant Scheduler
    participant UniverseManager
    participant KIS_API
    participant FileSystem

    Note over Scheduler: Daily 05:00 (Before Market Open)
    
    Scheduler->>UniverseManager: create_daily_snapshot("20260120")
    UniverseManager->>UniverseManager: calculate_prev_trading_day()
    Note over UniverseManager: Monday -> Friday (-3 days)<br/>Tue~Fri -> Previous day (-1 day)
    
    UniverseManager->>KIS_API: fetch_all_symbols()
    KIS_API-->>UniverseManager: ~2,500 symbols
    
    loop For each symbol (batch 100)
        UniverseManager->>KIS_API: fetch_eod_price(symbol, prev_day)
        KIS_API-->>UniverseManager: {close: 71100, volume: ...}
        Note over UniverseManager: Wait 1s between batches<br/>(Rate Limit compliance)
    end
    
    UniverseManager->>UniverseManager: filter(close >= 4000)
    UniverseManager->>UniverseManager: sort_by_market_cap(desc)
    
    UniverseManager->>FileSystem: save_snapshot(config/universe/20260120_kr_stocks.json)
    Note over UniverseManager: Expected: 800~1,200 symbols
    
    UniverseManager->>UniverseManager: update_cache(20260120, symbols)
    UniverseManager-->>Scheduler: snapshot_path
```

#### Selection Criteria

| Í∏∞Ï§Ä | Í∞í | Í∑ºÍ±∞ |
|-----|-----|-----|
| **Ï†ÑÏùº Ï¢ÖÍ∞Ä** | ‚â• 4,000Ïõê | Ïú†ÎèôÏÑ± ÌôïÎ≥¥, Ïä¨Î¶¨ÌîºÏßÄ ÏµúÏÜåÌôî |
| **ÏãúÍ∞ÄÏ¥ùÏï° Ï†ïÎ†¨** | ÎÇ¥Î¶ºÏ∞®Ïàú | ÎåÄÌòïÏ£º Ïö∞ÏÑ† Î∞∞Ïπò |
| **Í±∞ÎûòÏ†ïÏßÄ Ï¢ÖÎ™©** | Ï†úÏô∏ | Í±∞Îûò Î∂àÍ∞ÄÎä• Ï¢ÖÎ™© Î∞∞Ï†ú |
| **Í¥ÄÎ¶¨Ï¢ÖÎ™©** | Ï†úÏô∏ (ÏòµÏÖò) | Î¶¨Ïä§ÌÅ¨ ÌöåÌîº |

#### File Structure

**ÌååÏùº Í≤ΩÎ°ú:**
```
config/universe/{YYYYMMDD}_{market}.json
```

**ÌååÏùº Ìè¨Îß∑:**
```json
{
  "metadata": {
    "date": "20260120",
    "previous_trading_day": "20260119",
    "generated_at": "2026-01-20T05:00:00Z",
    "symbol_count": 1024,
    "market": "kr_stocks",
    "filter_criteria": "close_price >= 4000 KRW",
    "version": "1.0"
  },
  "symbols": [
    {
      "code": "005930",
      "name": "ÏÇºÏÑ±Ï†ÑÏûê",
      "market_cap": 4500000000000,
      "prev_close": 71100,
      "avg_volume_20d": 15000000
    },
    {
      "code": "000660",
      "name": "SKÌïòÏù¥ÎãâÏä§",
      "market_cap": 1200000000000,
      "prev_close": 145000,
      "avg_volume_20d": 3000000
    }
    // ... 1000+ symbols
  ]
}
```

### Universe Size Constraints

| Ìï≠Î™© | Í∞í | ÏÑ§Î™Ö |
|-----|-----|-----|
| **ÏòàÏÉÅ ÌÅ¨Í∏∞** | 800 ~ 1,200 Ï¢ÖÎ™© | KRX Í∏∞Ï§Ä Ï†ÑÏùº Ï¢ÖÍ∞Ä 4,000Ïõê Ïù¥ÏÉÅ |
| **ÏµúÏÜå ÌóàÏö©** | 100 Ï¢ÖÎ™© | Ïù¥Ìïò Ïãú Í≤ΩÍ≥† Î∞úÏÉù |
| **ÏµúÎåÄ Ï†úÌïú** | ÏóÜÏùå | Track AÍ∞Ä Ï†ÑÏ≤¥ Ïª§Î≤Ñ |
| **Track A Coverage** | 100% | Universe Ï†ÑÏ≤¥ Ï¢ÖÎ™© 10Î∂ÑÎßàÎã§ ÏàòÏßë |
| **Track B Coverage** | 41 Ï¢ÖÎ™© | Ïã§ÏãúÍ∞Ñ WebSocket Ï†úÌïú |

### Universe Update Policy

| Ï†ïÏ±Ö | ÏÑ§Ï†ï |
|-----|-----|
| **ÏÉùÏÑ± ÏãúÏ†ê** | Îß§Ïùº 05:00 (Ïû• ÏãúÏûë Ï†Ñ) |
| **ÎãπÏùº Í≥†Ï†ï** | Yes (Ïû¨ÌòÑÏÑ± Î≥¥Ïû•) |
| **Ïã§Ìå® Ïãú Ï≤òÎ¶¨** | Ï†ÑÏùº Universe Ïû¨ÏÇ¨Ïö© + ÏïåÎ¶º |
| **Ï∫êÏãú Í¥ÄÎ¶¨** | Î©îÎ™®Î¶¨ Ï∫êÏãú Ïú†ÏßÄ (ÏµúÍ∑º 10Ïùº) |
| **Î∞±ÏóÖ** | ÏùºÏùº Î∞±ÏóÖ Ìå®ÌÇ§ÏßÄ Ìè¨Ìï® |

---

## Slot Management

### Slot Concept

**Slot**ÏùÄ Track B WebSocketÏùÑ ÌÜµÌï¥ **Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ**Ìï† Ïàò ÏûàÎäî Ï¢ÖÎ™© ÏúÑÏπòÏûÖÎãàÎã§. KIS API WebSocket Ï†úÏïΩÏúºÎ°ú **ÏµúÎåÄ 41Í∞ú ÎèôÏãú Íµ¨ÎèÖ**Ïù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            WebSocket Connection (KIS)               ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ        41 Subscription Slots (Fixed)         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  [Slot 1] 005930  ‚îÄ‚îÄ‚ñ∫ Tick Stream (2Hz)     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  [Slot 2] 000660  ‚îÄ‚îÄ‚ñ∫ Tick Stream (2Hz)     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  [Slot 3] 035420  ‚îÄ‚îÄ‚ñ∫ Tick Stream (2Hz)     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ...                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  [Slot 41] 051910 ‚îÄ‚îÄ‚ñ∫ Tick Stream (2Hz)     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  ‚ö†Ô∏è Overflow Candidates (42nd+)                    ‚îÇ
‚îÇ  ‚Üí Recorded in Ledger (not subscribed)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Slot Allocation Strategy

#### Priority Levels

Ïä¨Î°Ø Ìï†ÎãπÏùÄ **Trigger Priority**Ïóê Îî∞Îùº Í≤∞Ï†ïÎê©ÎãàÎã§:

| Ïö∞ÏÑ†ÏàúÏúÑ | Trigger Type | ÏÑ§Î™Ö | Weight |
|---------|-------------|------|--------|
| **P1** | Volume Surge | Í±∞ÎûòÎüâ Í∏âÏ¶ù (ÌèâÍ∑† ÎåÄÎπÑ 3Î∞∞‚Üë) | 10 |
| **P2** | Trade Velocity | Ï≤¥Í≤∞ ÏÜçÎèÑ Ï¶ùÍ∞Ä (1Ï¥à 10Í±¥‚Üë) | 8 |
| **P3** | Volatility Spike | Î≥ÄÎèôÏÑ± Í∏âÏ¶ù (ATR ÎåÄÎπÑ 2Î∞∞‚Üë) | 6 |
| **P4** | Manual Override | ÏàòÎèô ÏßÄÏ†ï | 5 |
| **P5** | Market Cap | ÏãúÍ∞ÄÏ¥ùÏï° (Í∏∞Î≥∏ Î∞∞Ïπò) | 1 |

#### Allocation Algorithm

```python
def allocate_slots(candidates: List[Candidate], max_slots: int = 41) -> AllocationResult:
    """
    Ïä¨Î°Ø Ìï†Îãπ ÏïåÍ≥†Î¶¨Ï¶ò
    
    Args:
        candidates: ÌõÑÎ≥¥ Ï¢ÖÎ™© Î¶¨Ïä§Ìä∏ (trigger Ï†ïÎ≥¥ Ìè¨Ìï®)
        max_slots: ÏµúÎåÄ Ïä¨Î°Ø Ïàò (Í∏∞Î≥∏: 41)
    
    Returns:
        AllocationResult(allocated, overflow)
    """
    # Step 1: Ïö∞ÏÑ†ÏàúÏúÑ Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞
    scored = []
    for candidate in candidates:
        score = calculate_priority_score(candidate)
        scored.append((candidate, score))
    
    # Step 2: Ïä§ÏΩîÏñ¥ ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨
    scored.sort(key=lambda x: x[1], reverse=True)
    
    # Step 3: Top 41 Ìï†Îãπ, ÎÇòÎ®∏ÏßÄÎäî Overflow
    allocated = [c for c, s in scored[:max_slots]]
    overflow = [c for c, s in scored[max_slots:]]
    
    return AllocationResult(
        allocated=allocated,
        overflow=overflow,
        timestamp=datetime.now(timezone.utc)
    )


def calculate_priority_score(candidate: Candidate) -> float:
    """
    Ïö∞ÏÑ†ÏàúÏúÑ Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞
    
    Score = trigger_weight √ó trigger_strength + market_cap_factor
    """
    trigger_weights = {
        TriggerType.VOLUME_SURGE: 10.0,
        TriggerType.TRADE_VELOCITY: 8.0,
        TriggerType.VOLATILITY_SPIKE: 6.0,
        TriggerType.MANUAL: 5.0,
    }
    
    base_score = trigger_weights.get(candidate.trigger_type, 1.0)
    strength = candidate.trigger_strength  # 0.0 ~ 1.0
    market_cap_factor = math.log10(candidate.market_cap) * 0.1
    
    return base_score * (1 + strength) + market_cap_factor
```

### Slot Lifecycle

```mermaid
stateDiagram-v2
    [*] --> Available: System Start
    Available --> Allocated: allocate_slot()
    Allocated --> Active: subscribe_websocket()
    Active --> Releasing: release_slot()
    Releasing --> Available: unsubscribe_websocket()
    
    Active --> Replacing: Higher priority trigger
    Replacing --> Allocated: New symbol allocated
    
    note right of Available
        Empty slot ready
        for allocation
    end note
    
    note right of Active
        WebSocket subscribed
        Receiving tick data
    end note
    
    note right of Replacing
        Graceful replacement:
        1. Unsubscribe old
        2. Subscribe new
        3. Log transition
    end note
```

### Slot Replacement Policy

#### Replacement Conditions

Ïä¨Î°Ø ÍµêÏ≤¥Îäî Îã§Ïùå Ï°∞Í±¥ÏóêÏÑú Î∞úÏÉùÌï©ÎãàÎã§:

1. **Higher Priority Trigger**: ÌòÑÏû¨ Ïä¨Î°ØÎ≥¥Îã§ ÎÜíÏùÄ Ïö∞ÏÑ†ÏàúÏúÑ Ïù¥Î≤§Ìä∏ Î∞úÏÉù
2. **Expired Trigger**: Ìä∏Î¶¨Í±∞ Ïú†Ìö®Í∏∞Í∞Ñ ÎßåÎ£å (Í∏∞Î≥∏: 5Î∂Ñ)
3. **Manual Override**: ÏàòÎèô ÍµêÏ≤¥ ÏöîÏ≤≠
4. **Symbol Removal**: Ï¢ÖÎ™© Í±∞ÎûòÏ†ïÏßÄ/ÏÉÅÌïúÍ∞Ä Îì±

#### Replacement Algorithm

```python
def evaluate_replacement(
    active_slots: List[Slot],
    new_candidate: Candidate
) -> Optional[Slot]:
    """
    ÍµêÏ≤¥ ÎåÄÏÉÅ Ïä¨Î°Ø ÌèâÍ∞Ä
    
    Returns:
        ÍµêÏ≤¥ ÎåÄÏÉÅ Slot ÎòêÎäî None (ÍµêÏ≤¥ Î∂àÌïÑÏöî)
    """
    new_score = calculate_priority_score(new_candidate)
    
    # ÌòÑÏû¨ Ïä¨Î°Ø Ï§ë Í∞ÄÏû• ÎÇÆÏùÄ Ïä§ÏΩîÏñ¥ Ï∞æÍ∏∞
    min_slot = min(
        active_slots,
        key=lambda s: calculate_priority_score(s.candidate)
    )
    
    min_score = calculate_priority_score(min_slot.candidate)
    
    # ÏÉà ÌõÑÎ≥¥Í∞Ä Í∏∞Ï°¥ ÏµúÏÜå Ïä¨Î°ØÎ≥¥Îã§ Ï∂©Î∂ÑÌûà ÎÜíÏùÄ Ïä§ÏΩîÏñ¥Ïù∏Í∞Ä?
    if new_score > min_score * 1.2:  # 20% ÏûÑÍ≥ÑÍ∞í
        return min_slot
    
    return None
```

### Overflow Handling

41Í∞ú Ïä¨Î°ØÏùÑ Ï¥àÍ≥ºÌïòÎäî Ï¢ÖÎ™©ÏùÄ **Overflow**Î°ú Ï≤òÎ¶¨Îê©ÎãàÎã§.

#### Overflow Ledger

Overflow Ï¢ÖÎ™©ÏùÄ Î≥ÑÎèÑ LedgerÏóê Í∏∞Î°ùÌïòÏó¨ Ï∂îÏ†ÅÌï©ÎãàÎã§:

**ÌååÏùº Í≤ΩÎ°ú:**
```
data/observer/system/overflow/{provider}/YYYYMMDD_overflow.jsonl
```

**Î†àÏΩîÎìú Ìè¨Îß∑:**
```json
{
  "timestamp": "2026-01-20T10:35:22.123Z",
  "symbol": "035720",
  "trigger_type": "VOLUME_SURGE",
  "trigger_strength": 0.85,
  "priority_score": 9.2,
  "reason": "All 41 slots occupied by higher priority",
  "current_slot_min_score": 9.5,
  "metadata": {
    "volume_ratio": 3.2,
    "market_cap": 500000000000
  }
}
```

#### Overflow Monitoring

Overflow Î∞úÏÉùÎ•†ÏùÑ Î™®ÎãàÌÑ∞ÎßÅÌïòÏó¨ ÏãúÏä§ÌÖú Ïö©Îüâ Í≥ÑÌöçÏóê ÌôúÏö©Ìï©ÎãàÎã§:

| Metric | Threshold | Action |
|--------|-----------|--------|
| **Overflow Rate** | > 5% (1ÏãúÍ∞Ñ) | Í≤ΩÍ≥† Î°úÍ∑∏ |
| **Overflow Rate** | > 10% (1ÏãúÍ∞Ñ) | ÏïåÎ¶º Î∞úÏÜ° |
| **Overflow Rate** | > 20% (1ÏãúÍ∞Ñ) | Í∏¥Í∏â Í≤ÄÌÜ† ÌïÑÏöî |

---

## Trigger-based Selection

### Trigger Types

Track B Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ ÎåÄÏÉÅ Ï¢ÖÎ™©ÏùÄ **Trigger** Í∏∞Î∞òÏúºÎ°ú ÏÑ†Ï†ïÎê©ÎãàÎã§.

#### 1. Volume Surge (Í±∞ÎûòÎüâ Í∏âÏ¶ù)

**Ï†ïÏùò**: ÏµúÍ∑º Í±∞ÎûòÎüâÏù¥ ÌèâÍ∑† ÎåÄÎπÑ Í∏âÏ¶ùÌïú Í≤ΩÏö∞

**Í∞êÏßÄ Î°úÏßÅ:**
```python
def detect_volume_surge(symbol: str, current_volume: int) -> Optional[Trigger]:
    """
    Í±∞ÎûòÎüâ Í∏âÏ¶ù Í∞êÏßÄ
    
    Condition: current_volume >= avg_volume_20d * 3.0
    """
    avg_volume = get_avg_volume_20d(symbol)
    ratio = current_volume / avg_volume
    
    if ratio >= 3.0:
        strength = min(ratio / 5.0, 1.0)  # Normalize to 0~1
        return Trigger(
            type=TriggerType.VOLUME_SURGE,
            symbol=symbol,
            strength=strength,
            metadata={'ratio': ratio, 'avg_volume': avg_volume}
        )
    
    return None
```

**ÏûÑÍ≥ÑÍ∞í:**
- **Í≤ΩÎØ∏ (Mild)**: 2.0Î∞∞ Ïù¥ÏÉÅ
- **Ï§ëÍ∞Ñ (Moderate)**: 3.0Î∞∞ Ïù¥ÏÉÅ ‚úÖ **Í∏∞Î≥∏**
- **Ïã¨Í∞Å (Severe)**: 5.0Î∞∞ Ïù¥ÏÉÅ

#### 2. Trade Velocity (Ï≤¥Í≤∞ ÏÜçÎèÑ)

**Ï†ïÏùò**: Îã®ÏúÑ ÏãúÍ∞ÑÎãπ Ï≤¥Í≤∞ Í±¥ÏàòÍ∞Ä Í∏âÏ¶ùÌïú Í≤ΩÏö∞

**Í∞êÏßÄ Î°úÏßÅ:**
```python
def detect_trade_velocity(symbol: str, trades_per_sec: float) -> Optional[Trigger]:
    """
    Ï≤¥Í≤∞ ÏÜçÎèÑ Í∏âÏ¶ù Í∞êÏßÄ
    
    Condition: trades_per_sec >= 10
    """
    if trades_per_sec >= 10:
        strength = min(trades_per_sec / 20.0, 1.0)
        return Trigger(
            type=TriggerType.TRADE_VELOCITY,
            symbol=symbol,
            strength=strength,
            metadata={'trades_per_sec': trades_per_sec}
        )
    
    return None
```

**ÏûÑÍ≥ÑÍ∞í:**
- **Í∏∞Î≥∏ ÏûÑÍ≥ÑÍ∞í**: 10 Í±¥/Ï¥à
- **ÎÜíÏùÄ ÏûÑÍ≥ÑÍ∞í**: 20 Í±¥/Ï¥à

#### 3. Volatility Spike (Î≥ÄÎèôÏÑ± Í∏âÏ¶ù)

**Ï†ïÏùò**: Í∞ÄÍ≤© Î≥ÄÎèôÏÑ±Ïù¥ ÌèâÍ∑† ÎåÄÎπÑ Í∏âÏ¶ùÌïú Í≤ΩÏö∞

**Í∞êÏßÄ Î°úÏßÅ:**
```python
def detect_volatility_spike(symbol: str, current_atr: float) -> Optional[Trigger]:
    """
    Î≥ÄÎèôÏÑ± Í∏âÏ¶ù Í∞êÏßÄ
    
    Condition: current_atr >= avg_atr_14d * 2.0
    """
    avg_atr = get_avg_atr_14d(symbol)
    ratio = current_atr / avg_atr
    
    if ratio >= 2.0:
        strength = min(ratio / 3.0, 1.0)
        return Trigger(
            type=TriggerType.VOLATILITY_SPIKE,
            symbol=symbol,
            strength=strength,
            metadata={'ratio': ratio, 'current_atr': current_atr}
        )
    
    return None
```

**ÏûÑÍ≥ÑÍ∞í:**
- **Í∏∞Î≥∏ ÏûÑÍ≥ÑÍ∞í**: ATR 14Ïùº ÌèâÍ∑† ÎåÄÎπÑ 2.0Î∞∞

#### 4. Manual Override (ÏàòÎèô ÏßÄÏ†ï)

**Ï†ïÏùò**: ÏàòÎèôÏúºÎ°ú ÌäπÏ†ï Ï¢ÖÎ™©ÏùÑ Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ ÏßÄÏ†ï

**ÏÇ¨Ïö© ÏºÄÏù¥Ïä§:**
- ÌäπÏ†ï Ï¢ÖÎ™© ÏßëÏ§ë Î™®ÎãàÌÑ∞ÎßÅ ÌïÑÏöî Ïãú
- ÌÖåÏä§Ìä∏ Î∞è Í≤ÄÏ¶ù Î™©Ï†Å
- Í∏¥Í∏â Ïù¥Î≤§Ìä∏ ÎåÄÏùë

### Trigger Evaluation Cycle

```mermaid
sequenceDiagram
    participant TrackA as Track A Collector
    participant TriggerEngine as Trigger Engine
    participant SlotManager as Slot Manager
    participant TrackB as Track B Collector

    Note over TrackA: 10min interval snapshot
    
    TrackA->>TriggerEngine: evaluate_universe(snapshot)
    
    loop For each symbol in Universe
        TriggerEngine->>TriggerEngine: detect_volume_surge()
        TriggerEngine->>TriggerEngine: detect_trade_velocity()
        TriggerEngine->>TriggerEngine: detect_volatility_spike()
    end
    
    TriggerEngine->>TriggerEngine: rank_by_priority()
    TriggerEngine->>SlotManager: request_allocation(top_candidates)
    
    SlotManager->>SlotManager: evaluate_current_slots()
    SlotManager->>SlotManager: evaluate_replacements()
    
    alt Slot available
        SlotManager->>TrackB: subscribe(new_symbol)
    else Replacement needed
        SlotManager->>TrackB: unsubscribe(old_symbol)
        SlotManager->>TrackB: subscribe(new_symbol)
    else All slots occupied (higher priority)
        SlotManager->>SlotManager: record_overflow(candidate)
    end
    
    SlotManager-->>TriggerEngine: allocation_result
```

### Trigger Expiration

TriggerÎäî **Ïú†Ìö®Í∏∞Í∞Ñ**ÏùÑ Í∞ÄÏßÄÎ©∞, ÎßåÎ£å Ïãú Ïä¨Î°Ø Ïû¨ÌèâÍ∞ÄÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§.

| Trigger Type | Default TTL | Renewal Condition |
|-------------|-------------|-------------------|
| **Volume Surge** | 5Î∂Ñ | Volume ratio Ïú†ÏßÄ Ïãú Í∞±Ïã† |
| **Trade Velocity** | 3Î∂Ñ | Trade rate Ïú†ÏßÄ Ïãú Í∞±Ïã† |
| **Volatility Spike** | 5Î∂Ñ | Volatility ratio Ïú†ÏßÄ Ïãú Í∞±Ïã† |
| **Manual Override** | Î¨¥Ï†úÌïú | ÏàòÎèô Ìï¥Ï†ú ÏãúÍπåÏßÄ Ïú†ÏßÄ |



### Current Limitation: 41 Slots per Account

KIS API WebSocketÏùò **ÎèôÏãú Íµ¨ÎèÖ Ï†úÌïú 41Í∞ú**Îäî Îã®Ïùº Í≥ÑÏ†ïÏùò Î¨ºÎ¶¨Ï†Å ÌïúÍ≥ÑÏûÖÎãàÎã§. Ïù¥Î•º Í∑πÎ≥µÌïòÍ∏∞ ÏúÑÌï¥ **Î©ÄÌã∞ Í≥ÑÏ†ï Ï†ÑÎûµ**ÏùÑ ÎèÑÏûÖÌï©ÎãàÎã§.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Single Account (Current)                       ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   App Key 1                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ Slot 1  ~ Slot 41  (41 symbols)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Overflow: 42nd+  ‚ùå                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  Total Capacity: 41 symbols                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

            ‚Üì  Multi-Account Strategy

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Multi-Account (Future)                         ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   App Key 1                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Slot 1  ~ Slot 41  (41 symbols)        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   App Key 2                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Slot 42 ~ Slot 82  (41 symbols)        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   App Key 3                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Slot 83 ~ Slot 123 (41 symbols)        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  Total Capacity: 123 symbols (3 accounts)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Provider Pool Architecture

**Slot Abstract Layer**Î•º ÎèÑÏûÖÌïòÏó¨ Îã§Ï§ë App KeyÎ•º Ï∂îÏÉÅÌôîÌï©ÎãàÎã§.

```python
from dataclasses import dataclass
from typing import List, Optional, Dict
import asyncio
import logging

logger = logging.getLogger("ProviderPool")


@dataclass
class ProviderAccount:
    """Îã®Ïùº Provider Í≥ÑÏ†ï Ï†ïÎ≥¥"""
    account_id: str
    app_key: str
    app_secret: str
    approval_key: str
    token: Optional[str] = None
    max_slots: int = 41
    used_slots: int = 0
    
    def available_slots(self) -> int:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïä¨Î°Ø Ïàò"""
        return self.max_slots - self.used_slots
    
    def can_allocate(self, count: int = 1) -> bool:
        """Ïä¨Î°Ø Ìï†Îãπ Í∞ÄÎä• Ïó¨Î∂Ä"""
        return self.available_slots() >= count


class ProviderPool:
    """
    Î©ÄÌã∞ Í≥ÑÏ†ï Provider Pool
    
    Ïó¨Îü¨ Í∞úÏùò KIS App KeyÎ•º Í¥ÄÎ¶¨ÌïòÍ≥† Ïä¨Î°ØÏùÑ ÏûêÎèôÏúºÎ°ú Î∂ÑÏÇ∞Ìï©ÎãàÎã§.
    
    Features:
    - Í≥ÑÏ†ïÎ≥Ñ Ïä¨Î°Ø ÏÇ¨Ïö©Îüâ Ï∂îÏ†Å
    - ÎùºÏö¥Îìú Î°úÎπà Ìï†Îãπ Ï†ÑÎûµ
    - Í≥ÑÏ†ïÎ≥Ñ Health Monitoring
    - Í≥ÑÏ†ï Ïû•Ïï† Ïãú ÏûêÎèô Failover
    """
    
    def __init__(self, accounts: List[ProviderAccount]):
        if not accounts:
            raise ValueError("At least one account required")
        
        self.accounts = {acc.account_id: acc for acc in accounts}
        self.account_order = list(self.accounts.keys())
        self.current_index = 0
        
        # WebSocket ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÌíÄ (Í≥ÑÏ†ïÎ≥Ñ)
        self.ws_clients: Dict[str, KISWebSocketClient] = {}
        
        logger.info(f"Provider pool initialized with {len(accounts)} accounts")
        logger.info(f"Total capacity: {self.total_capacity()} slots")
    
    def total_capacity(self) -> int:
        """Ï†ÑÏ≤¥ Ïä¨Î°Ø Ïö©Îüâ"""
        return sum(acc.max_slots for acc in self.accounts.values())
    
    def total_used(self) -> int:
        """ÏÇ¨Ïö© Ï§ëÏù∏ Ïä¨Î°Ø Ïàò"""
        return sum(acc.used_slots for acc in self.accounts.values())
    
    def total_available(self) -> int:
        """ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïä¨Î°Ø Ïàò"""
        return self.total_capacity() - self.total_used()
    
    def select_account_for_allocation(self, count: int = 1) -> Optional[ProviderAccount]:
        """
        Ïä¨Î°Ø Ìï†ÎãπÏùÑ ÏúÑÌïú Í≥ÑÏ†ï ÏÑ†ÌÉù (Round-robin)
        
        Args:
            count: ÌïÑÏöîÌïú Ïä¨Î°Ø Ïàò
        
        Returns:
            ÏÑ†ÌÉùÎêú Í≥ÑÏ†ï ÎòêÎäî None (Ìï†Îãπ Î∂àÍ∞Ä)
        """
        # 2Ìöå ÏàúÌöå ÏãúÎèÑ (Î™®Îì† Í≥ÑÏ†ï Ï≤¥ÌÅ¨)
        for _ in range(len(self.account_order) * 2):
            account_id = self.account_order[self.current_index]
            account = self.accounts[account_id]
            
            if account.can_allocate(count):
                # Îã§Ïùå ÏÑ†ÌÉùÏùÑ ÏúÑÌï¥ Ïù∏Îç±Ïä§ Ï¶ùÍ∞Ä
                self.current_index = (self.current_index + 1) % len(self.account_order)
                return account
            
            self.current_index = (self.current_index + 1) % len(self.account_order)
        
        # Ìï†Îãπ Í∞ÄÎä•Ìïú Í≥ÑÏ†ï ÏóÜÏùå
        return None
    
    async def allocate_slot(self, symbol: str, candidate: Candidate) -> Optional[str]:
        """
        Ïä¨Î°Ø Ìï†Îãπ (ÏûêÎèô Í≥ÑÏ†ï ÏÑ†ÌÉù)
        
        Returns:
            Ìï†ÎãπÎêú Ïä¨Î°Ø ID (format: "{account_id}:{slot_num}")
        """
        account = self.select_account_for_allocation(count=1)
        
        if not account:
            logger.warning(f"No available slots for {symbol} (total capacity full)")
            return None
        
        # WebSocket ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Í∞ÄÏ†∏Ïò§Í∏∞
        ws_client = self.ws_clients.get(account.account_id)
        
        if not ws_client:
            logger.error(f"WebSocket client not found for account {account.account_id}")
            return None
        
        # WebSocket Íµ¨ÎèÖ
        try:
            await ws_client.subscribe(symbol)
            account.used_slots += 1
            
            slot_id = f"{account.account_id}:{account.used_slots}"
            
            logger.info(
                f"Slot allocated: {slot_id} for {symbol} "
                f"(Account: {account.account_id}, "
                f"Used: {account.used_slots}/{account.max_slots})"
            )
            
            return slot_id
            
        except Exception as e:
            logger.error(f"Failed to allocate slot for {symbol}: {e}")
            return None
    
    async def release_slot(self, slot_id: str, symbol: str):
        """
        Ïä¨Î°Ø Ìï¥Ï†ú
        
        Args:
            slot_id: Ïä¨Î°Ø ID (format: "{account_id}:{slot_num}")
            symbol: Ï¢ÖÎ™© ÏΩîÎìú
        """
        account_id = slot_id.split(':')[0]
        account = self.accounts.get(account_id)
        
        if not account:
            logger.error(f"Account not found: {account_id}")
            return
        
        ws_client = self.ws_clients.get(account_id)
        
        if not ws_client:
            logger.error(f"WebSocket client not found for account {account_id}")
            return
        
        try:
            await ws_client.unsubscribe(symbol)
            account.used_slots = max(0, account.used_slots - 1)
            
            logger.info(
                f"Slot released: {slot_id} for {symbol} "
                f"(Account: {account_id}, "
                f"Used: {account.used_slots}/{account.max_slots})"
            )
            
        except Exception as e:
            logger.error(f"Failed to release slot {slot_id}: {e}")
    
    async def initialize_all_connections(self):
        """
        Î™®Îì† Í≥ÑÏ†ïÏùò WebSocket Ïó∞Í≤∞ Ï¥àÍ∏∞Ìôî
        """
        logger.info("Initializing WebSocket connections for all accounts...")
        
        tasks = []
        for account in self.accounts.values():
            task = self._initialize_account_connection(account)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        
        logger.info(
            f"WebSocket initialization complete: "
            f"{success_count}/{len(self.accounts)} accounts connected"
        )
    
    async def _initialize_account_connection(self, account: ProviderAccount):
        """
        Îã®Ïùº Í≥ÑÏ†ïÏùò WebSocket Ïó∞Í≤∞ Ï¥àÍ∏∞Ìôî
        """
        logger.info(f"Initializing WebSocket for account: {account.account_id}")
        
        try:
            ws_client = KISWebSocketClient(
                app_key=account.app_key,
                app_secret=account.app_secret,
                approval_key=account.approval_key
            )
            
            await ws_client.connect()
            
            self.ws_clients[account.account_id] = ws_client
            
            logger.info(f"‚úÖ Account {account.account_id} connected")
            
        except Exception as e:
            logger.error(
                f"Failed to initialize account {account.account_id}: {e}",
                exc_info=True
            )
            raise
    
    async def health_check_all(self) -> Dict[str, bool]:
        """
        Î™®Îì† Í≥ÑÏ†ï Health Check
        
        Returns:
            {account_id: is_healthy}
        """
        health_status = {}
        
        for account_id, ws_client in self.ws_clients.items():
            try:
                is_healthy = await ws_client.ping()
                health_status[account_id] = is_healthy
            except:
                health_status[account_id] = False
        
        unhealthy = [aid for aid, status in health_status.items() if not status]
        
        if unhealthy:
            logger.warning(f"Unhealthy accounts detected: {unhealthy}")
        
        return health_status
    
    def get_utilization_stats(self) -> Dict:
        """
        Ïä¨Î°Ø ÏÇ¨Ïö©Î•† ÌÜµÍ≥Ñ
        """
        return {
            'total_capacity': self.total_capacity(),
            'total_used': self.total_used(),
            'total_available': self.total_available(),
            'utilization_rate': self.total_used() / self.total_capacity(),
            'accounts': [
                {
                    'account_id': acc.account_id,
                    'used': acc.used_slots,
                    'capacity': acc.max_slots,
                    'utilization': acc.used_slots / acc.max_slots
                }
                for acc in self.accounts.values()
            ]
        }


class EnhancedSlotManager:
    """
    Provider Pool ÌÜµÌï© Slot Manager
    
    Í∏∞Ï°¥ SlotManagerÎ•º ÌôïÏû•ÌïòÏó¨ Provider PoolÍ≥º ÌÜµÌï©Ìï©ÎãàÎã§.
    """
    
    def __init__(self, provider_pool: ProviderPool, overflow_logger):
        self.provider_pool = provider_pool
        self.overflow_logger = overflow_logger
        
        # Slot ID to Symbol mapping
        self.slot_map: Dict[str, str] = {}  # {slot_id: symbol}
        
    async def allocate(self, candidates: List[Candidate]) -> AllocationResult:
        """
        ÌõÑÎ≥¥ Î¶¨Ïä§Ìä∏ÏóêÏÑú Ïä¨Î°Ø Ìï†Îãπ (Provider Pool ÏÇ¨Ïö©)
        """
        allocated = []
        overflow = []
        replacements = []
        
        for candidate in candidates:
            symbol = candidate.symbol.code
            
            # Check if already allocated
            if symbol in self.slot_map.values():
                logger.debug(f"Symbol {symbol} already allocated")
                continue
            
            # Try to allocate from provider pool
            slot_id = await self.provider_pool.allocate_slot(symbol, candidate)
            
            if slot_id:
                self.slot_map[slot_id] = symbol
                allocated.append(candidate)
            else:
                # Check if replacement possible
                replacement_slot = await self._evaluate_replacement(candidate)
                
                if replacement_slot:
                    # Release old slot
                    old_symbol = self.slot_map[replacement_slot]
                    await self.provider_pool.release_slot(replacement_slot, old_symbol)
                    
                    # Allocate new slot
                    new_slot_id = await self.provider_pool.allocate_slot(symbol, candidate)
                    
                    if new_slot_id:
                        del self.slot_map[replacement_slot]
                        self.slot_map[new_slot_id] = symbol
                        replacements.append((replacement_slot, candidate))
                        allocated.append(candidate)
                else:
                    # Overflow
                    overflow.append(candidate)
                    self.overflow_logger.record(candidate)
        
        return AllocationResult(
            allocated=allocated,
            overflow=overflow,
            replacements=replacements,
            timestamp=datetime.now(timezone.utc)
        )
    
    async def _evaluate_replacement(self, new_candidate: Candidate) -> Optional[str]:
        """
        ÍµêÏ≤¥ ÎåÄÏÉÅ Ïä¨Î°Ø ÌèâÍ∞Ä
        
        Returns:
            ÍµêÏ≤¥Ìï† slot_id ÎòêÎäî None
        """
        # Get all current slots with their candidates
        # (Implementation depends on how candidates are tracked)
        # For now, return None (no replacement)
        return None
```

### Multi-Account Configuration

**ÏÑ§Ï†ï ÌååÏùº**: `config/provider_pool.json`

```json
{
  "provider_pool": {
    "accounts": [
      {
        "account_id": "account_001",
        "app_key": "PSxxxxxxxxxxxxxxxxxxxxxx",
        "app_secret": "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
        "max_slots": 41,
        "priority": 1
      },
      {
        "account_id": "account_002",
        "app_key": "PSyyyyyyyyyyyyyyyyyyyyyy",
        "app_secret": "yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy",
        "max_slots": 41,
        "priority": 2
      },
      {
        "account_id": "account_003",
        "app_key": "PSzzzzzzzzzzzzzzzzzzzzzz",
        "app_secret": "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz",
        "max_slots": 41,
        "priority": 3
      }
    ],
    "allocation_strategy": "round_robin",
    "health_check_interval_seconds": 60,
    "failover_enabled": true
  }
}
```

### Scalability Roadmap

| Phase | Accounts | Capacity | Target Date |
|-------|----------|----------|-------------|
| **Phase 1** | 1 account | 41 slots | Current |
| **Phase 2** | 3 accounts | 123 slots | Q2 2026 |
| **Phase 3** | 5 accounts | 205 slots | Q3 2026 |
| **Phase 4** | 10 accounts | 410 slots | Q4 2026 |

### Cost Analysis

| Accounts | Monthly Cost | Cost per Slot | Notes |
|----------|-------------|---------------|-------|
| 1 | $0 (Free) | $0 | KIS API Î¨¥Î£å |
| 3 | $0 (Free) | $0 | Í≥ÑÏ†ï ÏÉùÏÑ± Î¨¥Î£å |
| 5 | $0 (Free) | $0 | Îã®, Í¥ÄÎ¶¨ ÎπÑÏö© Ï¶ùÍ∞Ä |
| 10 | $0 (Free) | $0 | Ïö¥ÏòÅ Î≥µÏû°ÎèÑ ÎÜíÏùå |

**Note**: KIS APIÎäî Î¨¥Î£åÏù¥ÎÇò, Îã§Ï§ë Í≥ÑÏ†ï Í¥ÄÎ¶¨ Î≥µÏû°ÎèÑ Î∞è Î™®ÎãàÌÑ∞ÎßÅ ÎπÑÏö© Í≥†Î†§ ÌïÑÏöî

---

## Data Continuity and Gap Recovery

### Problem Statement

WebSocket Ïû¨Ïó∞Í≤∞ Ïãú Î∞úÏÉùÌïòÎäî Îç∞Ïù¥ÌÑ∞ Í≥µÎ∞±ÏùÄ Î∞±ÌÖåÏä§ÌåÖ Î∞è Ï†ÑÎûµ Í≤ÄÏ¶ù Ïãú Ïã¨Í∞ÅÌïú Î¨∏Ï†úÎ•º ÏïºÍ∏∞Ìï©ÎãàÎã§. ÌäπÌûà Ïû¨Ïó∞Í≤∞ ÏÜåÏöî ÏãúÍ∞Ñ(ÌèâÍ∑† 2~5Ï¥à)ÎèôÏïà Î∞úÏÉùÌïú Ìã± Îç∞Ïù¥ÌÑ∞Îäî ÏòÅÍµ¨Ï†ÅÏúºÎ°ú ÏÜêÏã§Îê©ÎãàÎã§.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         WebSocket Disconnection Scenario              ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  09:15:30  Last tick received                         ‚îÇ
‚îÇ  09:15:31  ‚ùå Connection lost                         ‚îÇ
‚îÇ  09:15:32  üîÑ Reconnecting... (backoff)              ‚îÇ
‚îÇ  09:15:34  ‚úÖ Reconnected                             ‚îÇ
‚îÇ  09:15:35  First tick received                        ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  ‚ö†Ô∏è GAP: 09:15:31 ~ 09:15:34 (4 seconds)           ‚îÇ
‚îÇ     - Missing ticks: ~8 ticks (2Hz √ó 4s)             ‚îÇ
‚îÇ     - Missing data: OHLC, Volume                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Gap Detection Mechanism

```python
import asyncio
from datetime import datetime, timedelta, timezone
from dataclasses import dataclass
from typing import Optional, List
import logging

logger = logging.getLogger("GapRecovery")


@dataclass
class DataGap:
    """Îç∞Ïù¥ÌÑ∞ Í∞≠ Ï†ïÎ≥¥"""
    symbol: str
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    reason: str  # "websocket_reconnect", "token_refresh", etc.
    

class GapDetector:
    """
    Îç∞Ïù¥ÌÑ∞ Í∞≠ Í∞êÏßÄ Î∞è Í∏∞Î°ù
    """
    
    def __init__(self, expected_interval_ms: float = 500):
        # Expected interval: 500ms (2Hz)
        self.expected_interval = timedelta(milliseconds=expected_interval_ms)
        self.tolerance = timedelta(milliseconds=200)  # 20% tolerance
        self.last_tick_time: dict[str, datetime] = {}  # symbol -> last_time
        
    def check_gap(self, symbol: str, current_time: datetime) -> Optional[DataGap]:
        """
        Í∞≠ Ïó¨Î∂Ä ÌôïÏù∏
        
        Returns:
            DataGap if gap detected, None otherwise
        """
        if symbol not in self.last_tick_time:
            self.last_tick_time[symbol] = current_time
            return None
        
        last_time = self.last_tick_time[symbol]
        time_diff = current_time - last_time
        
        # Gap detected if time_diff > expected + tolerance
        threshold = self.expected_interval + self.tolerance
        
        if time_diff > threshold:
            gap = DataGap(
                symbol=symbol,
                start_time=last_time,
                end_time=current_time,
                duration_seconds=time_diff.total_seconds(),
                reason="unknown"  # Set by caller
            )
            
            logger.warning(
                f"Gap detected: {symbol} "
                f"from {last_time} to {current_time} "
                f"({gap.duration_seconds:.2f}s)"
            )
            
            self.last_tick_time[symbol] = current_time
            return gap
        
        self.last_tick_time[symbol] = current_time
        return None
    
    def mark_expected_gap(self, symbols: List[str], reason: str):
        """
        ÏòàÏÉÅÎêú Í∞≠ ÎßàÌÇπ (Ïû¨Ïó∞Í≤∞ ÏãúÏûë Ïãú Ìò∏Ï∂ú)
        """
        now = datetime.now(timezone.utc)
        for symbol in symbols:
            if symbol in self.last_tick_time:
                self.last_tick_time[symbol] = now
        
        logger.info(f"Expected gap marked for {len(symbols)} symbols: {reason}")


class GapRecoveryEngine:
    """
    Gap Recovery Engine
    
    WebSocket Ïû¨Ïó∞Í≤∞ Ïãú Î∞úÏÉùÌïú Îç∞Ïù¥ÌÑ∞ Í≥µÎ∞±ÏùÑ Track A (REST API)Î•º ÌÜµÌï¥ Î≥¥Ï†ïÌï©ÎãàÎã§.
    """
    
    def __init__(self, rest_client, gap_logger):
        self.rest_client = rest_client
        self.gap_logger = gap_logger
        self.recovery_queue: asyncio.Queue = asyncio.Queue()
        
    async def start_recovery_worker(self):
        """
        Gap recovery worker (Î∞±Í∑∏ÎùºÏö¥Îìú Ïã§Ìñâ)
        """
        logger.info("Gap recovery worker started")
        
        while True:
            try:
                gap = await self.recovery_queue.get()
                await self._recover_gap(gap)
                self.recovery_queue.task_done()
            except Exception as e:
                logger.error(f"Gap recovery error: {e}", exc_info=True)
                await asyncio.sleep(1)
    
    async def enqueue_gap(self, gap: DataGap):
        """
        GapÏùÑ Î≥µÍµ¨ ÌÅêÏóê Ï∂îÍ∞Ä
        """
        await self.recovery_queue.put(gap)
        logger.info(f"Gap enqueued for recovery: {gap.symbol}")
    
    async def _recover_gap(self, gap: DataGap):
        """
        Gap Î≥µÍµ¨ Ïã§Ìñâ
        
        Strategy:
        1. REST APIÎ°ú Ìï¥Îãπ ÏãúÍ∞Ñ Íµ¨Í∞ÑÏùò Î∂ÑÎ¥â Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå
        2. Î∂ÑÎ¥â Îç∞Ïù¥ÌÑ∞Î°ú OHLC, Volume Î≥¥Ï†ï
        3. Î≥µÍµ¨Îêú Îç∞Ïù¥ÌÑ∞ Í∏∞Î°ù (Î≥ÑÎèÑ recovery ÌååÏùº)
        """
        logger.info(
            f"Starting gap recovery for {gap.symbol} "
            f"({gap.start_time} ~ {gap.end_time})"
        )
        
        try:
            # Step 1: Fetch minute bar data
            # KIS API: Î∂ÑÎ¥â Ï°∞ÌöåÎäî ÏùºÎ¥â APIÏôÄ Ïú†ÏÇ¨ÌïòÍ≤å ÏÇ¨Ïö©
            minute_data = await self._fetch_minute_bars(
                symbol=gap.symbol,
                start_time=gap.start_time,
                end_time=gap.end_time
            )
            
            if not minute_data:
                logger.warning(f"No minute data available for {gap.symbol}")
                await self._record_unrecoverable_gap(gap)
                return
            
            # Step 2: Create recovery records
            recovery_records = self._create_recovery_records(
                gap=gap,
                minute_data=minute_data
            )
            
            # Step 3: Write to recovery file
            await self._write_recovery_records(
                gap=gap,
                records=recovery_records
            )
            
            logger.info(
                f"‚úÖ Gap recovered for {gap.symbol}: "
                f"{len(recovery_records)} records"
            )
            
        except Exception as e:
            logger.error(
                f"Failed to recover gap for {gap.symbol}: {e}",
                exc_info=True
            )
            await self._record_unrecoverable_gap(gap)
    
    async def _fetch_minute_bars(self, symbol: str, start_time: datetime, 
                                  end_time: datetime) -> List[dict]:
        """
        REST APIÎ°ú Î∂ÑÎ¥â Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå
        
        Note: KIS APIÎäî Î∂ÑÎ¥â ÏßÅÏ†ë Ï°∞ÌöåÍ∞Ä Ï†úÌïúÏ†ÅÏùº Ïàò ÏûàÏùå
              Ïù¥ Í≤ΩÏö∞ ÎãπÏùº Ï†ÑÏ≤¥ Ìã± Îç∞Ïù¥ÌÑ∞Î•º Ï°∞ÌöåÌïòÏó¨ ÌïÑÌÑ∞ÎßÅ
        """
        # Simplified: ÌòÑÏû¨Í∞Ä Ï°∞ÌöåÎ°ú ÎåÄÏ≤¥ (Ïã§Ï†úÎ°úÎäî Î∂ÑÎ¥â API ÏÇ¨Ïö©)
        try:
            response = await self.rest_client.fetch_price(symbol)
            
            if response:
                return [{
                    'timestamp': end_time,
                    'open': response['output']['stck_oprc'],
                    'high': response['output']['stck_hgpr'],
                    'low': response['output']['stck_lwpr'],
                    'close': response['output']['stck_prpr'],
                    'volume': response['output']['acml_vol']
                }]
            
            return []
            
        except Exception as e:
            logger.error(f"Failed to fetch minute bars: {e}")
            return []
    
    def _create_recovery_records(self, gap: DataGap, 
                                  minute_data: List[dict]) -> List[dict]:
        """
        Î≥µÍµ¨ Î†àÏΩîÎìú ÏÉùÏÑ±
        """
        records = []
        
        for bar in minute_data:
            record = {
                'meta': {
                    'source': 'gap_recovery',
                    'recovery_method': 'rest_api_minute_bar',
                    'original_gap': {
                        'start': gap.start_time.isoformat(),
                        'end': gap.end_time.isoformat(),
                        'duration_seconds': gap.duration_seconds,
                        'reason': gap.reason
                    },
                    'recovered_at': datetime.now(timezone.utc).isoformat()
                },
                'instruments': [{
                    'symbol': gap.symbol,
                    'timestamp': bar['timestamp'].isoformat(),
                    'price': {
                        'open': float(bar['open']),
                        'high': float(bar['high']),
                        'low': float(bar['low']),
                        'close': float(bar['close'])
                    },
                    'volume': int(bar['volume']),
                    'data_quality': 'recovered'
                }]
            }
            records.append(record)
        
        return records
    
    async def _write_recovery_records(self, gap: DataGap, records: List[dict]):
        """
        Î≥µÍµ¨ Î†àÏΩîÎìúÎ•º Î≥ÑÎèÑ ÌååÏùºÏóê Í∏∞Î°ù
        
        ÌååÏùº Í≤ΩÎ°ú: data/observer/scalp/{provider}/{market}/YYYYMMDD/recovery_HHMMSS.jsonl
        """
        from pathlib import Path
        import json
        
        date_str = gap.start_time.strftime("%Y%m%d")
        time_str = gap.start_time.strftime("%H%M%S")
        
        recovery_dir = Path(
            f"data/observer/scalp/kis/kr_stocks/{date_str}"
        )
        recovery_dir.mkdir(parents=True, exist_ok=True)
        
        recovery_file = recovery_dir / f"recovery_{time_str}_{gap.symbol}.jsonl"
        
        with open(recovery_file, 'a', encoding='utf-8') as f:
            for record in records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
        
        logger.info(f"Recovery records written to: {recovery_file}")
    
    async def _record_unrecoverable_gap(self, gap: DataGap):
        """
        Î≥µÍµ¨ Î∂àÍ∞ÄÎä•Ìïú Í∞≠ Í∏∞Î°ù
        """
        await self.gap_logger.record({
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'gap_type': 'unrecoverable',
            'symbol': gap.symbol,
            'start_time': gap.start_time.isoformat(),
            'end_time': gap.end_time.isoformat(),
            'duration_seconds': gap.duration_seconds,
            'reason': gap.reason
        })
```

### Gap Recovery Integration

**WebSocket Reconnection HandlerÏóê ÌÜµÌï©:**

```python
class WebSocketManager:
    """WebSocket Manager with Gap Recovery"""
    
    def __init__(self, gap_detector, gap_recovery_engine):
        self.gap_detector = gap_detector
        self.gap_recovery = gap_recovery_engine
        self.active_symbols: List[str] = []
        
    async def handle_reconnection(self, reason: str):
        """
        Ïû¨Ïó∞Í≤∞ Ï≤òÎ¶¨ with Gap Recovery
        """
        logger.info(f"WebSocket reconnection initiated: {reason}")
        
        # Mark expected gap for all active symbols
        self.gap_detector.mark_expected_gap(
            symbols=self.active_symbols,
            reason=reason
        )
        
        reconnect_start = datetime.now(timezone.utc)
        
        # Perform reconnection
        await self._reconnect()
        
        reconnect_end = datetime.now(timezone.utc)
        reconnect_duration = (reconnect_end - reconnect_start).total_seconds()
        
        logger.info(f"Reconnection completed in {reconnect_duration:.2f}s")
        
        # Enqueue gaps for recovery
        for symbol in self.active_symbols:
            gap = DataGap(
                symbol=symbol,
                start_time=reconnect_start,
                end_time=reconnect_end,
                duration_seconds=reconnect_duration,
                reason=reason
            )
            await self.gap_recovery.enqueue_gap(gap)
    
    async def on_tick_received(self, symbol: str, tick_data: dict):
        """
        Ìã± ÏàòÏã† Ï≤òÎ¶¨ with Gap Detection
        """
        current_time = datetime.fromisoformat(tick_data['timestamp'])
        
        # Check for unexpected gap
        gap = self.gap_detector.check_gap(symbol, current_time)
        
        if gap:
            # Unexpected gap detected
            gap.reason = "unexpected_gap"
            logger.warning(f"Unexpected gap detected: {gap}")
            await self.gap_recovery.enqueue_gap(gap)
        
        # Normal tick processing
        await self._process_tick(symbol, tick_data)
```

### Gap Recovery Policy

| Gap Duration | Recovery Strategy | Priority |
|-------------|------------------|----------|
| **< 5Ï¥à** | REST API Ï¶âÏãú Î≥µÍµ¨ | HIGH |
| **5 ~ 60Ï¥à** | REST API ÌÅê Î≥µÍµ¨ (ÎπÑÎèôÍ∏∞) | MEDIUM |
| **> 60Ï¥à** | Gap MarkerÎßå Í∏∞Î°ù (Î≥µÍµ¨ Î∂àÍ∞Ä) | LOW |
| **30Î∂Ñ (09:00~09:30)** | Context Sync at Startup | CRITICAL |

### Recovery Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| **Detection Latency** | < 1Ï¥à | Í∞≠ Í∞êÏßÄ ÏãúÍ∞Ñ |
| **Recovery Latency** | < 5Ï¥à | REST API Ìò∏Ï∂ú ~ Í∏∞Î°ù |
| **Success Rate** | > 95% | Î≥µÍµ¨ ÏÑ±Í≥µÎ•† |
| **Queue Depth** | < 100 | ÎåÄÍ∏∞ Ï§ëÏù∏ Î≥µÍµ¨ ÏûëÏóÖ |
| **Context Sync Time** | < 30Ï¥à | 09:30 Í∏∞Îèô Ïãú Ïª®ÌÖçÏä§Ìä∏ ÎèôÍ∏∞Ìôî |

### Track B Context Synchronization (v1.2 NEW)

#### Problem: Cold Start at 09:30

Track BÍ∞Ä 09:30Ïóê ÏãúÏûëÌïòÎ©¥ 09:00~09:30 Íµ¨Í∞ÑÏùò ÏãúÏû• Ïª®ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏñ¥ Îã§Ïùå Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§:

1. **Historical Context Loss**: Ïû• Ï¥àÎ∞ò Í∞ÄÍ≤© Î≥ÄÎèô Ïù¥Î†• Î∂ÄÏû¨
2. **Trigger Misfire**: 09:30 Ïù¥Ï†Ñ Í±∞ÎûòÎüâ/Î≥ÄÎèôÏÑ± Í∏∞Ï§ÄÏÑ† Î∂ÄÏû¨
3. **Cold Start Bias**: Ï¥àÍ∏∞ Ïä¨Î°Ø Ìï†ÎãπÏù¥ Î∂ÄÏ†ïÌôïÌï† Ïàò ÏûàÏùå

#### Solution: Pre-Startup Context Synchronization

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Track B Startup Sequence (09:30)               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  09:29:50  Pre-startup initiated                        ‚îÇ
‚îÇ             ‚Üì                                            ‚îÇ
‚îÇ  09:29:51  [1/4] Fetch 09:00~09:30 Track A data        ‚îÇ
‚îÇ             - Query recent snapshots from Track A        ‚îÇ
‚îÇ             - Extract OHLC, Volume for active universe   ‚îÇ
‚îÇ             ‚Üì                                            ‚îÇ
‚îÇ  09:29:55  [2/4] Calculate baseline metrics             ‚îÇ
‚îÇ             - Compute 30-min avg volume                  ‚îÇ
‚îÇ             - Compute 30-min volatility                  ‚îÇ
‚îÇ             - Identify early movers                      ‚îÇ
‚îÇ             ‚Üì                                            ‚îÇ
‚îÇ  09:29:58  [3/4] Pre-select initial slots               ‚îÇ
‚îÇ             - Rank candidates by opening activity        ‚îÇ
‚îÇ             - Allocate top 41 slots                      ‚îÇ
‚îÇ             ‚Üì                                            ‚îÇ
‚îÇ  09:30:00  [4/4] Start WebSocket streaming             ‚îÇ
‚îÇ             - Subscribe to pre-selected symbols          ‚îÇ
‚îÇ             - Begin 2Hz tick collection                  ‚îÇ
‚îÇ             ‚úÖ Context-aware operation                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Implementation

```python
import asyncio
from datetime import datetime, timedelta, timezone
from typing import List, Dict
import logging

logger = logging.getLogger("ContextSync")


class TrackBContextSynchronizer:
    """
    Track B Í∏∞Îèô Ïãú 09:00~09:30 Ïª®ÌÖçÏä§Ìä∏ ÎèôÍ∏∞Ìôî
    
    Purpose:
    - Fetch opening 30-min data from Track A
    - Calculate baseline metrics (volume, volatility)
    - Pre-select initial slots based on opening activity
    - Enable context-aware real-time monitoring
    """
    
    def __init__(self, track_a_reader, trigger_engine, slot_manager):
        self.track_a_reader = track_a_reader
        self.trigger_engine = trigger_engine
        self.slot_manager = slot_manager
        
    async def synchronize_context(self, target_time: datetime) -> Dict:
        """
        Ïª®ÌÖçÏä§Ìä∏ ÎèôÍ∏∞Ìôî Ïã§Ìñâ
        
        Args:
            target_time: Track B Í∏∞Îèô ÏãúÍ∞Å (ÏùºÎ∞òÏ†ÅÏúºÎ°ú 09:30)
        
        Returns:
            {
                'baseline_metrics': {...},
                'initial_slots': [...],
                'sync_duration': float
            }
        """
        start_time = datetime.now(timezone.utc)
        
        logger.info("=" * 60)
        logger.info("TRACK B CONTEXT SYNCHRONIZATION INITIATED")
        logger.info(f"Target time: {target_time.strftime('%H:%M:%S')}")
        logger.info("=" * 60)
        
        try:
            # Step 1: Fetch opening period data (09:00~09:30)
            logger.info("[1/4] Fetching opening period data (09:00~09:30)...")
            opening_period_start = target_time.replace(
                hour=9, minute=0, second=0, microsecond=0
            )
            
            opening_data = await self._fetch_opening_data(
                start=opening_period_start,
                end=target_time
            )
            
            logger.info(
                f"[1/4] Fetched {len(opening_data)} symbols from Track A"
            )
            
            # Step 2: Calculate baseline metrics
            logger.info("[2/4] Calculating baseline metrics...")
            baseline_metrics = self._calculate_baseline_metrics(opening_data)
            
            logger.info(
                f"[2/4] Baseline calculated: "
                f"avg_volume={baseline_metrics['avg_volume']:.0f}, "
                f"avg_volatility={baseline_metrics['avg_volatility']:.4f}"
            )
            
            # Step 3: Pre-select initial slots
            logger.info("[3/4] Pre-selecting initial slots...")
            initial_candidates = self._rank_opening_movers(
                opening_data,
                baseline_metrics
            )
            
            # Allocate top 41 slots
            initial_slots = await self.slot_manager.allocate(
                initial_candidates[:41]
            )
            
            logger.info(
                f"[3/4] Pre-selected {len(initial_slots.allocated)} slots"
            )
            
            # Step 4: Store context for trigger engine
            self.trigger_engine.set_baseline_metrics(baseline_metrics)
            
            sync_duration = (datetime.now(timezone.utc) - start_time).total_seconds()
            
            logger.info("[4/4] Context synchronization complete")
            logger.info(f"‚úÖ Sync completed in {sync_duration:.2f}s")
            logger.info("Track B ready to start with context awareness")
            
            return {
                'baseline_metrics': baseline_metrics,
                'initial_slots': [s.symbol.code for s in initial_slots.allocated],
                'sync_duration': sync_duration,
                'timestamp': target_time.isoformat()
            }
            
        except Exception as e:
            logger.error(
                f"Context synchronization failed: {e}",
                exc_info=True
            )
            
            # Fallback: Start without context (degraded mode)
            logger.warning(
                "‚ö†Ô∏è Starting Track B without context (degraded mode)"
            )
            
            return {
                'baseline_metrics': None,
                'initial_slots': [],
                'sync_duration': 0,
                'error': str(e)
            }
    
    async def _fetch_opening_data(
        self,
        start: datetime,
        end: datetime
    ) -> List[Dict]:
        """
        Track AÏóêÏÑú Ïû• Ï¥àÎ∞ò Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå
        
        Returns:
            [
                {
                    'symbol': '005930',
                    'open': 71000,
                    'high': 71500,
                    'low': 70800,
                    'close': 71200,
                    'volume': 1500000,
                    'timestamp': '2026-01-20T09:30:00+09:00'
                },
                ...
            ]
        """
        # Track AÎäî 10Î∂Ñ Ï£ºÍ∏∞Ïù¥ÎØÄÎ°ú 09:00, 09:10, 09:20, 09:30 Îç∞Ïù¥ÌÑ∞ Ï°¥Ïû¨
        # ÏµúÍ∑º 3Í∞ú Ïä§ÎÉÖÏÉ∑ Ï°∞Ìöå
        snapshots = await self.track_a_reader.query_snapshots(
            start_time=start,
            end_time=end,
            limit=3
        )
        
        # Ï¢ÖÎ™©Î≥ÑÎ°ú ÏßëÍ≥Ñ
        symbol_data = {}
        
        for snapshot in snapshots:
            for instrument in snapshot.get('instruments', []):
                symbol = instrument['symbol']
                
                if symbol not in symbol_data:
                    symbol_data[symbol] = {
                        'symbol': symbol,
                        'snapshots': []
                    }
                
                symbol_data[symbol]['snapshots'].append({
                    'timestamp': snapshot['meta']['captured_at'],
                    'price': instrument['price'],
                    'volume': instrument['volume']
                })
        
        # OHLCV Í≥ÑÏÇ∞
        opening_data = []
        
        for symbol, data in symbol_data.items():
            prices = [s['price']['close'] for s in data['snapshots']]
            volumes = [s['volume'] for s in data['snapshots']]
            
            opening_data.append({
                'symbol': symbol,
                'open': prices[0] if prices else 0,
                'high': max(prices) if prices else 0,
                'low': min(prices) if prices else 0,
                'close': prices[-1] if prices else 0,
                'volume': sum(volumes) if volumes else 0,
                'timestamp': end.isoformat()
            })
        
        return opening_data
    
    def _calculate_baseline_metrics(self, opening_data: List[Dict]) -> Dict:
        """
        Í∏∞Ï§ÄÏÑ† Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        
        Returns:
            {
                'avg_volume': float,
                'avg_volatility': float,
                'volume_threshold': float,
                'volatility_threshold': float
            }
        """
        if not opening_data:
            return {
                'avg_volume': 0,
                'avg_volatility': 0,
                'volume_threshold': 0,
                'volatility_threshold': 0
            }
        
        # ÌèâÍ∑† Í±∞ÎûòÎüâ
        avg_volume = sum(d['volume'] for d in opening_data) / len(opening_data)
        
        # ÌèâÍ∑† Î≥ÄÎèôÏÑ± (High-Low / Open)
        volatilities = []
        for d in opening_data:
            if d['open'] > 0:
                volatility = (d['high'] - d['low']) / d['open']
                volatilities.append(volatility)
        
        avg_volatility = sum(volatilities) / len(volatilities) if volatilities else 0
        
        # ÏûÑÍ≥ÑÍ∞í ÏÑ§Ï†ï (ÌèâÍ∑†Ïùò 1.5Î∞∞)
        volume_threshold = avg_volume * 1.5
        volatility_threshold = avg_volatility * 1.5
        
        return {
            'avg_volume': avg_volume,
            'avg_volatility': avg_volatility,
            'volume_threshold': volume_threshold,
            'volatility_threshold': volatility_threshold,
            'sample_count': len(opening_data)
        }
    
    def _rank_opening_movers(self, opening_data: List[Dict],
                              baseline: Dict) -> List[Candidate]:
        """
        Ïû• Ï¥àÎ∞ò ÌôúÎ∞úÌïú Ï¢ÖÎ™© ÏàúÏúÑ Îß§Í∏∞Í∏∞
        
        Criteria:
        1. Volume ratio (vs baseline)
        2. Volatility ratio (vs baseline)
        3. Price change magnitude
        """
        ranked = []
        
        for data in opening_data:
            # Volume ratio
            volume_ratio = (
                data['volume'] / baseline['avg_volume']
                if baseline['avg_volume'] > 0 else 0
            )
            
            # Volatility ratio
            volatility = (
                (data['high'] - data['low']) / data['open']
                if data['open'] > 0 else 0
            )
            volatility_ratio = (
                volatility / baseline['avg_volatility']
                if baseline['avg_volatility'] > 0 else 0
            )
            
            # Price change
            price_change = (
                abs(data['close'] - data['open']) / data['open']
                if data['open'] > 0 else 0
            )
            
            # Composite score
            score = (
                volume_ratio * 0.4 +
                volatility_ratio * 0.3 +
                price_change * 100 * 0.3
            )
            
            ranked.append({
                'data': data,
                'score': score,
                'volume_ratio': volume_ratio,
                'volatility_ratio': volatility_ratio,
                'price_change': price_change
            })
        
        # Sort by score (descending)
        ranked.sort(key=lambda x: x['score'], reverse=True)
        
        # Convert to Candidate objects
        candidates = []
        for item in ranked:
            # Create mock Candidate (actual implementation depends on your Candidate class)
            candidate = Candidate(
                symbol=Symbol(code=item['data']['symbol'], name="", market_cap=0),
                trigger=Trigger(
                    type=TriggerType.VOLUME_SURGE,
                    symbol=item['data']['symbol'],
                    strength=min(item['volume_ratio'] / 3.0, 1.0),
                    detected_at=datetime.now(timezone.utc),
                    ttl_seconds=300,
                    metadata={'source': 'context_sync'}
                ),
                priority_score=item['score']
            )
            candidates.append(candidate)
        
        return candidates
```

#### Integration with Track B Startup

```python
class TrackBRunner:
    """Track B Runner with Context Synchronization"""
    
    async def run_async(self):
        """Track B Î©îÏù∏ Î£®ÌîÑ with pre-startup context sync"""
        logger.info("Track B Runner started (dedicated process)")
        
        # Wait until 09:29:50
        await self._wait_until_pre_startup()
        
        # Context synchronization (09:29:50 ~ 09:30:00)
        context_sync = TrackBContextSynchronizer(
            track_a_reader=track_a_reader,
            trigger_engine=trigger_engine,
            slot_manager=slot_manager
        )
        
        target_time = datetime.now().replace(hour=9, minute=30, second=0)
        sync_result = await context_sync.synchronize_context(target_time)
        
        if sync_result.get('error'):
            logger.warning(
                "Context sync failed, starting without baseline"
            )
        else:
            logger.info(
                f"Context sync successful: "
                f"{len(sync_result['initial_slots'])} initial slots"
            )
        
        # Start WebSocket at 09:30:00
        await self._start_websocket_streaming()
        
        # Main loop (09:30 ~ 15:00)
        while self._is_operation_time():
            # Normal tick processing
            await self._process_ticks()
        
        logger.info("Track B Runner stopped at 15:00")
    
    def _is_operation_time(self) -> bool:
        """09:30 ~ 15:00 Íµ¨Í∞ÑÏù∏ÏßÄ ÌôïÏù∏"""
        now = datetime.now()
        
        # Check if between 09:30 and 15:00
        start_time = now.replace(hour=9, minute=30, second=0)
        end_time = now.replace(hour=15, minute=0, second=0)
        
        return start_time <= now < end_time
```

---

## Resource Isolation Architecture

### Problem: Track A/B Interference

ÌòÑÏû¨ Îã®Ïùº ÌîÑÎ°úÏÑ∏Ïä§ Íµ¨Ï°∞ÏóêÏÑúÎäî Track A (10Î∂Ñ Ï£ºÍ∏∞ Î≤åÌÅ¨ ÏàòÏßë)ÏôÄ Track B (2Hz Ïã§ÏãúÍ∞Ñ ÏàòÏßë)Í∞Ä ÎèôÏùºÌïú Ïù¥Î≤§Ìä∏ Î£®ÌîÑÎ•º Í≥µÏú†ÌïòÏó¨ Îã§Ïùå Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌï† Ïàò ÏûàÏäµÎãàÎã§:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Single Process (Current)                       ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ      Main Event Loop (asyncio)               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Track A    ‚îÇ  ‚îÇ     Track B        ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  (10min)     ‚îÇ  ‚îÇ     (2Hz)          ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  üîÑ Bulk    ‚îÇ  ‚îÇ  ‚ö° Real-time     ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Collection  ‚îÇ  ‚îÇ  Streaming         ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                   ‚îÇ               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                     ‚Üì                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ            ‚ö†Ô∏è Resource Contention          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ            - CPU blocking                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ            - I/O congestion                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ            - GIL contention                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Î¨∏Ï†úÏ†ê**:
1. **Track AÏùò ÎåÄÎüâ API Ìò∏Ï∂úÏù¥ Track B ÏßÄÏó∞ Ïú†Î∞ú**
2. **Track BÏùò ÎÜíÏùÄ ÎπàÎèÑÍ∞Ä Track A ÌÉÄÏûÑÏïÑÏõÉ Ïú†Î∞ú**
3. **Í≥µÏú† GILÎ°ú Ïù∏Ìïú CPU Î≥ëÎ™©**
4. **Í≥µÏú† I/O Î≤ÑÌçºÎ°ú Ïù∏Ìïú Ïì∞Í∏∞ ÏßÄÏó∞**

### Solution: Multi-Process Isolation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Multi-Process Architecture (v1.1)               ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Process 1        ‚îÇ      ‚îÇ   Process 2           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Track A Runner   ‚îÇ      ‚îÇ   Track B Runner      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ      ‚îÇ                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ      ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Event Loop  ‚îÇ ‚îÇ      ‚îÇ  ‚îÇ  Event Loop     ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  (asyncio)   ‚îÇ ‚îÇ      ‚îÇ  ‚îÇ  (asyncio)      ‚îÇ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ      ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ          ‚îÇ      ‚îÇ           ‚îÇ          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  üîÑ REST API      ‚îÇ      ‚îÇ  ‚ö° WebSocket       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  10min Bulk       ‚îÇ      ‚îÇ  2Hz Streaming      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                    ‚îÇ      ‚îÇ                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  CPU: 30-40%      ‚îÇ      ‚îÇ  CPU: 20-30%         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  MEM: 500MB       ‚îÇ      ‚îÇ  MEM: 300MB          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ            ‚îÇ                            ‚îÇ             ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                       ‚Üì                               ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ              ‚îÇ   IPC Channel    ‚îÇ                     ‚îÇ
‚îÇ              ‚îÇ  (Unix Socket /  ‚îÇ                     ‚îÇ
‚îÇ              ‚îÇ   Shared Queue)  ‚îÇ                     ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ                       ‚Üì                               ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ              ‚îÇ  Observer Core   ‚îÇ                     ‚îÇ
‚îÇ              ‚îÇ  (Main Process)  ‚îÇ                     ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation

```python
import multiprocessing as mp
from multiprocessing import Process, Queue, Event
import asyncio
import logging
from typing import Optional

logger = logging.getLogger("ResourceIsolation")


class TrackARunner:
    """
    Track A (REST API Î≤åÌÅ¨ ÏàòÏßë) Ï†ÑÏö© ÌîÑÎ°úÏÑ∏Ïä§
    """
    
    def __init__(self, config, output_queue: Queue, shutdown_event: Event):
        self.config = config
        self.output_queue = output_queue
        self.shutdown_event = shutdown_event
        
    async def run_async(self):
        """
        Track A Î©îÏù∏ Î£®ÌîÑ (ÎπÑÎèôÍ∏∞)
        """
        logger.info("Track A Runner started (dedicated process)")
        
        # Initialize components
        universe_manager = UniverseManager(provider_engine)
        rest_client = KISRestClient()
        
        while not self.shutdown_event.is_set():
            try:
                # 10Î∂ÑÎßàÎã§ Ïã§Ìñâ
                universe = universe_manager.get_current_universe("kr_stocks")
                
                logger.info(f"Track A: Collecting {len(universe)} symbols")
                
                # Î∞∞Ïπò Ï≤òÎ¶¨ (100Í∞úÏî©)
                for i in range(0, len(universe), 100):
                    if self.shutdown_event.is_set():
                        break
                    
                    batch = universe[i:i+100]
                    
                    # Î≥ëÎ†¨ ÏàòÏßë
                    tasks = [rest_client.fetch_price(s.code) for s in batch]
                    results = await asyncio.gather(
                        *tasks, 
                        return_exceptions=True
                    )
                    
                    # Í≤∞Í≥ºÎ•º ÌÅêÎ°ú Ï†ÑÏÜ°
                    for symbol, result in zip(batch, results):
                        if not isinstance(result, Exception):
                            self.output_queue.put({
                                'track': 'A',
                                'symbol': symbol.code,
                                'data': result
                            })
                    
                    # Rate limit Ï§ÄÏàò
                    await asyncio.sleep(15)
                
                logger.info("Track A: Collection cycle completed")
                
                # 10Î∂Ñ ÎåÄÍ∏∞
                for _ in range(600):  # 10Î∂Ñ = 600Ï¥à
                    if self.shutdown_event.is_set():
                        break
                    await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"Track A error: {e}", exc_info=True)
                await asyncio.sleep(60)
        
        logger.info("Track A Runner stopped")
    
    def run(self):
        """ÌîÑÎ°úÏÑ∏Ïä§ ÏóîÌä∏Î¶¨ Ìè¨Ïù∏Ìä∏ (ÎèôÍ∏∞)"""
        asyncio.run(self.run_async())


class TrackBRunner:
    """
    Track B (WebSocket Ïã§ÏãúÍ∞Ñ ÏàòÏßë) Ï†ÑÏö© ÌîÑÎ°úÏÑ∏Ïä§
    """
    
    def __init__(self, config, output_queue: Queue, shutdown_event: Event):
        self.config = config
        self.output_queue = output_queue
        self.shutdown_event = shutdown_event
        
    async def run_async(self):
        """
        Track B Î©îÏù∏ Î£®ÌîÑ (ÎπÑÎèôÍ∏∞)
        """
        logger.info("Track B Runner started (dedicated process)")
        
        # Initialize components
        ws_client = KISWebSocketClient()
        slot_manager = SlotManager(overflow_logger)
        
        # Connect WebSocket
        await ws_client.connect()
        
        # Subscribe to slots
        active_slots = slot_manager.get_active_symbols()
        for symbol in active_slots:
            await ws_client.subscribe(symbol)
        
        # Message processing loop
        while not self.shutdown_event.is_set():
            try:
                # Receive tick with timeout
                tick = await asyncio.wait_for(
                    ws_client.receive(),
                    timeout=1.0
                )
                
                # Send to main process via queue
                self.output_queue.put({
                    'track': 'B',
                    'symbol': tick['symbol'],
                    'data': tick
                })
                
            except asyncio.TimeoutError:
                # No data, continue
                continue
            except Exception as e:
                logger.error(f"Track B error: {e}", exc_info=True)
                await asyncio.sleep(1)
        
        # Cleanup
        await ws_client.disconnect()
        logger.info("Track B Runner stopped")
    
    def run(self):
        """ÌîÑÎ°úÏÑ∏Ïä§ ÏóîÌä∏Î¶¨ Ìè¨Ïù∏Ìä∏ (ÎèôÍ∏∞)"""
        asyncio.run(self.run_async())


class MultiProcessOrchestrator:
    """
    Î©ÄÌã∞ ÌîÑÎ°úÏÑ∏Ïä§ Ïò§ÏºÄÏä§Ìä∏Î†àÏù¥ÌÑ∞
    
    Track AÏôÄ Track BÎ•º ÎèÖÎ¶ΩÎêú ÌîÑÎ°úÏÑ∏Ïä§Î°ú Ïã§ÌñâÌïòÍ≥† IPCÎ°ú ÌÜµÏã†Ìï©ÎãàÎã§.
    """
    
    def __init__(self, config):
        self.config = config
        
        # IPC Ï±ÑÎÑê
        self.track_a_queue = mp.Queue(maxsize=10000)
        self.track_b_queue = mp.Queue(maxsize=10000)
        
        # Ï¢ÖÎ£å Ïù¥Î≤§Ìä∏
        self.shutdown_event = mp.Event()
        
        # ÌîÑÎ°úÏÑ∏Ïä§ Ïù∏Ïä§ÌÑ¥Ïä§
        self.track_a_process: Optional[Process] = None
        self.track_b_process: Optional[Process] = None
        
    def start(self):
        """
        Î™®Îì† ÌîÑÎ°úÏÑ∏Ïä§ ÏãúÏûë
        """
        logger.info("Starting multi-process architecture...")
        
        # Track A ÌîÑÎ°úÏÑ∏Ïä§
        self.track_a_process = Process(
            target=TrackARunner(
                config=self.config.track_a,
                output_queue=self.track_a_queue,
                shutdown_event=self.shutdown_event
            ).run,
            name="TrackA-Process"
        )
        self.track_a_process.start()
        logger.info(f"Track A process started: PID={self.track_a_process.pid}")
        
        # Track B ÌîÑÎ°úÏÑ∏Ïä§
        self.track_b_process = Process(
            target=TrackBRunner(
                config=self.config.track_b,
                output_queue=self.track_b_queue,
                shutdown_event=self.shutdown_event
            ).run,
            name="TrackB-Process"
        )
        self.track_b_process.start()
        logger.info(f"Track B process started: PID={self.track_b_process.pid}")
        
        # Î©îÏù∏ ÌîÑÎ°úÏÑ∏Ïä§: IPC Î©îÏãúÏßÄ Ï≤òÎ¶¨
        asyncio.run(self._message_processing_loop())
    
    async def _message_processing_loop(self):
        """
        IPC Î©îÏãúÏßÄ Ï≤òÎ¶¨ Î£®ÌîÑ (Î©îÏù∏ ÌîÑÎ°úÏÑ∏Ïä§)
        """
        logger.info("IPC message processing loop started")
        
        observer = Observer(
            session_id="observer_001",
            mode="production",
            event_bus=event_bus
        )
        observer.start()
        
        while not self.shutdown_event.is_set():
            try:
                # Track A ÌÅê ÌôïÏù∏ (non-blocking)
                while not self.track_a_queue.empty():
                    message = self.track_a_queue.get_nowait()
                    await self._process_track_a_message(observer, message)
                
                # Track B ÌÅê ÌôïÏù∏ (non-blocking)
                while not self.track_b_queue.empty():
                    message = self.track_b_queue.get_nowait()
                    await self._process_track_b_message(observer, message)
                
                # CPU ÏñëÎ≥¥
                await asyncio.sleep(0.01)
                
            except Exception as e:
                logger.error(f"Message processing error: {e}", exc_info=True)
                await asyncio.sleep(1)
        
        observer.stop()
        logger.info("IPC message processing loop stopped")
    
    async def _process_track_a_message(self, observer, message):
        """Track A Î©îÏãúÏßÄ Ï≤òÎ¶¨"""
        snapshot = create_swing_snapshot(message['data'])
        observer.on_snapshot(snapshot)
    
    async def _process_track_b_message(self, observer, message):
        """Track B Î©îÏãúÏßÄ Ï≤òÎ¶¨"""
        snapshot = create_scalp_snapshot(message['data'])
        observer.on_snapshot(snapshot)
    
    def stop(self):
        """
        Î™®Îì† ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å
        """
        logger.info("Stopping multi-process architecture...")
        
        # Ï¢ÖÎ£å ÏãúÍ∑∏ÎÑê
        self.shutdown_event.set()
        
        # ÌîÑÎ°úÏÑ∏Ïä§ Ï¢ÖÎ£å ÎåÄÍ∏∞
        if self.track_a_process:
            self.track_a_process.join(timeout=10)
            if self.track_a_process.is_alive():
                logger.warning("Track A process did not stop, terminating...")
                self.track_a_process.terminate()
        
        if self.track_b_process:
            self.track_b_process.join(timeout=10)
            if self.track_b_process.is_alive():
                logger.warning("Track B process did not stop, terminating...")
                self.track_b_process.terminate()
        
        logger.info("All processes stopped")
```

### Resource Isolation Benefits

| Aspect | Single Process | Multi-Process |
|--------|---------------|---------------|
| **GIL Contention** | High | None (separate GIL) |
| **CPU Isolation** | Shared | Dedicated per track |
| **Memory Isolation** | Shared | Separate heap |
| **I/O Blocking** | Mutual blocking | Independent |
| **Crash Isolation** | Full system crash | Track-specific crash |
| **Resource Monitoring** | Aggregated | Per-track metrics |

### Performance Comparison

| Metric | Single Process | Multi-Process | Improvement |
|--------|---------------|---------------|-------------|
| Track A Latency | 150~300ms | 100~150ms | **50% ‚Üì** |
| Track B Latency | 20~50ms | 10~20ms | **50% ‚Üì** |
| CPU Util (Track A) | 60% | 35% | **40% ‚Üì** |
| CPU Util (Track B) | 40% | 25% | **37% ‚Üì** |

### Operation Scheduler (v1.2 NEW)

#### Purpose

Í∞Å TrackÏùò ÏãúÏûë/Ï¢ÖÎ£å ÏãúÍ∞ÑÏùÑ Ï†ïÎ∞ÄÌïòÍ≤å Ï†úÏñ¥ÌïòÏó¨:
1. **Noise Filtering**: Ïû• Ï¥àÎ∞ò/ÎßàÍ∞ê ÎÖ∏Ïù¥Ï¶à Íµ¨Í∞Ñ Ï†úÏô∏
2. **Resource Optimization**: Î∂àÌïÑÏöîÌïú Íµ¨Í∞ÑÏùò Î¶¨ÏÜåÏä§ ÏÇ¨Ïö© Î∞©ÏßÄ
3. **Data Quality**: Í≥†ÌíàÏßà Îç∞Ïù¥ÌÑ∞Îßå ÏàòÏßë
4. **System Stability**: ÌîºÌÅ¨ ÏãúÍ∞ÑÎåÄ Î∂ÄÌïò Î∂ÑÏÇ∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Operation Scheduler Timeline (v1.2)            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  08:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ
‚îÇ            ‚îÇ  System Wake-up                             ‚îÇ
‚îÇ            ‚îÇ  - Token refresh                            ‚îÇ
‚îÇ            ‚îÇ  - System warmup                            ‚îÇ
‚îÇ            ‚îî‚îÄ‚ñ∫ [Ready]                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  09:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ
‚îÇ            ‚îÇ  Track A Start                              ‚îÇ
‚îÇ            ‚îÇ  - Full universe coverage                   ‚îÇ
‚îÇ            ‚îÇ  - 10min interval                           ‚îÇ
‚îÇ            ‚îÇ                                              ‚îÇ
‚îÇ            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ            ‚îî‚îÄ‚ñ∫‚îÇ    Track A Active           ‚îÇ           ‚îÇ
‚îÇ               ‚îÇ    (09:00~15:30)            ‚îÇ           ‚îÇ
‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  09:30 ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ
‚îÇ            ‚îÇ  Track B Start                              ‚îÇ
‚îÇ            ‚îÇ  - Context sync (09:00~09:30)               ‚îÇ
‚îÇ            ‚îÇ  - 41 slots allocation                      ‚îÇ
‚îÇ            ‚îÇ  - 2Hz real-time streaming                  ‚îÇ
‚îÇ            ‚îÇ                                              ‚îÇ
‚îÇ            ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Track B Active ‚îÇ                   ‚îÇ
‚îÇ                    ‚îÇ  (09:30~15:00)  ‚îÇ                   ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  15:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ
‚îÇ            ‚îÇ  Track B Stop                               ‚îÇ
‚îÇ            ‚îÇ  - Graceful shutdown                        ‚îÇ
‚îÇ            ‚îÇ  - Final data flush                         ‚îÇ
‚îÇ            ‚îî‚îÄ‚ñ∫ [Track B Inactive]                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  15:30 ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ
‚îÇ            ‚îÇ  Track A Stop                               ‚îÇ
‚îÇ            ‚îÇ  - Final snapshot collection                ‚îÇ
‚îÇ            ‚îî‚îÄ‚ñ∫ [Track A Inactive]                        ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  21:00 ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                            ‚îÇ
‚îÇ            ‚îÇ  Daily Backup                               ‚îÇ
‚îÇ            ‚îî‚îÄ‚ñ∫ [System Idle]                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Implementation

```python
import asyncio
from datetime import datetime, time, timezone, timedelta
from enum import Enum, auto
from typing import Optional, Callable, Dict
import logging

logger = logging.getLogger("OperationScheduler")


class OperationPhase(Enum):
    """Ïö¥ÏòÅ Îã®Í≥Ñ"""
    IDLE = auto()           # ÏãúÏä§ÌÖú Ïú†Ìú¥
    WARMUP = auto()         # ÏãúÏä§ÌÖú ÏòàÏó¥
    TRACK_A_ONLY = auto()   # Track AÎßå ÌôúÏÑ±
    FULL_OPERATION = auto() # Track A + B Î™®Îëê ÌôúÏÑ±
    TRACK_A_CLOSING = auto()# Track AÎßå ÌôúÏÑ± (B Ï¢ÖÎ£å)
    SHUTDOWN = auto()       # ÏãúÏä§ÌÖú Ï¢ÖÎ£å


class OperationScheduler:
    """
    Ïö¥ÏòÅ ÏãúÍ∞Ñ Í∏∞Î∞ò Ïä§ÏºÄÏ§ÑÎü¨
    
    Responsibilities:
    - Track A/BÏùò ÏãúÏûë/Ï¢ÖÎ£å ÏãúÍ∞Ñ Ï†úÏñ¥
    - Phase Ï†ÑÌôò Í¥ÄÎ¶¨
    - ÏãúÍ∞Ñ Í∏∞Î∞ò ÏûëÏóÖ Ìä∏Î¶¨Í±∞
    - Ïö¥ÏòÅ ÏãúÍ∞Ñ Í≤ÄÏ¶ù
    """
    
    def __init__(self, config: Dict):
        self.config = config
        
        # Operation times (KST)
        self.wake_up_time = time(8, 0)      # 08:00
        self.track_a_start = time(9, 0)     # 09:00
        self.track_b_start = time(9, 30)    # 09:30
        self.track_b_stop = time(15, 0)     # 15:00
        self.track_a_stop = time(15, 30)    # 15:30
        self.backup_time = time(21, 0)      # 21:00
        
        self.current_phase = OperationPhase.IDLE
        self.phase_callbacks: Dict[OperationPhase, Callable] = {}
        
    def register_phase_callback(self, phase: OperationPhase,
                                 callback: Callable):
        """
        Phase Ï†ÑÌôò Ïãú Ìò∏Ï∂úÌï† ÏΩúÎ∞± Îì±Î°ù
        
        Args:
            phase: ÎåÄÏÉÅ Phase
            callback: async function to call on phase transition
        """
        self.phase_callbacks[phase] = callback
        logger.info(f"Registered callback for phase: {phase.name}")
    
    async def start_scheduler(self):
        """
        Ïä§ÏºÄÏ§ÑÎü¨ ÏãúÏûë (Î¨¥Ìïú Î£®ÌîÑ)
        """
        logger.info("Operation Scheduler started")
        logger.info(f"Wake-up time: {self.wake_up_time}")
        logger.info(f"Track A: {self.track_a_start} ~ {self.track_a_stop}")
        logger.info(f"Track B: {self.track_b_start} ~ {self.track_b_stop}")
        
        while True:
            try:
                current_time = self._get_kst_time()
                new_phase = self._determine_phase(current_time)
                
                # Phase Ï†ÑÌôò Í∞êÏßÄ
                if new_phase != self.current_phase:
                    await self._transition_phase(
                        from_phase=self.current_phase,
                        to_phase=new_phase
                    )
                    self.current_phase = new_phase
                
                # 10Ï¥àÎßàÎã§ Ï≤¥ÌÅ¨
                await asyncio.sleep(10)
                
            except Exception as e:
                logger.error(f"Scheduler error: {e}", exc_info=True)
                await asyncio.sleep(60)
    
    def _get_kst_time(self) -> time:
        """ÌòÑÏû¨ KST ÏãúÍ∞Ñ Î∞òÌôò"""
        utc_now = datetime.now(timezone.utc)
        kst_now = utc_now + timedelta(hours=9)
        return kst_now.time()
    
    def _determine_phase(self, current_time: time) -> OperationPhase:
        """
        ÌòÑÏû¨ ÏãúÍ∞ÑÏóê Îî∞Î•∏ Phase Í≤∞Ï†ï
        
        Args:
            current_time: KST time
        
        Returns:
            OperationPhase
        """
        # 08:00 ~ 09:00: WARMUP
        if self.wake_up_time <= current_time < self.track_a_start:
            return OperationPhase.WARMUP
        
        # 09:00 ~ 09:30: TRACK_A_ONLY
        if self.track_a_start <= current_time < self.track_b_start:
            return OperationPhase.TRACK_A_ONLY
        
        # 09:30 ~ 15:00: FULL_OPERATION
        if self.track_b_start <= current_time < self.track_b_stop:
            return OperationPhase.FULL_OPERATION
        
        # 15:00 ~ 15:30: TRACK_A_CLOSING
        if self.track_b_stop <= current_time < self.track_a_stop:
            return OperationPhase.TRACK_A_CLOSING
        
        # 15:30 ~ 21:00: SHUTDOWN
        if self.track_a_stop <= current_time < self.backup_time:
            return OperationPhase.SHUTDOWN
        
        # 21:00 ~: IDLE
        return OperationPhase.IDLE
    
    async def _transition_phase(self, from_phase: OperationPhase,
                                 to_phase: OperationPhase):
        """
        Phase Ï†ÑÌôò Ï≤òÎ¶¨
        """
        logger.info("=" * 60)
        logger.info(f"PHASE TRANSITION: {from_phase.name} ‚Üí {to_phase.name}")
        logger.info(f"Time: {self._get_kst_time()}")
        logger.info("=" * 60)
        
        # Execute registered callback
        callback = self.phase_callbacks.get(to_phase)
        
        if callback:
            try:
                await callback()
                logger.info(f"‚úÖ Phase callback executed: {to_phase.name}")
            except Exception as e:
                logger.error(
                    f"Phase callback failed: {to_phase.name}",
                    exc_info=True
                )
        
        # Phase-specific actions
        if to_phase == OperationPhase.WARMUP:
            logger.info("üåÖ System wake-up: Token refresh & warmup")
        
        elif to_phase == OperationPhase.TRACK_A_ONLY:
            logger.info("üìä Track A started: Full market surveillance")
        
        elif to_phase == OperationPhase.FULL_OPERATION:
            logger.info(
                "‚ö° Track B started: Real-time monitoring (with context sync)"
            )
        
        elif to_phase == OperationPhase.TRACK_A_CLOSING:
            logger.info(
                "üõë Track B stopped: Intentional early shutdown "
                "(avoid closing noise)"
            )
        
        elif to_phase == OperationPhase.SHUTDOWN:
            logger.info("üì¥ Track A stopped: Market surveillance ended")
        
        elif to_phase == OperationPhase.IDLE:
            logger.info("üí§ System idle: Waiting for next day")
    
    def is_track_a_active(self) -> bool:
        """Track A ÌôúÏÑ± Ïó¨Î∂Ä"""
        return self.current_phase in [
            OperationPhase.TRACK_A_ONLY,
            OperationPhase.FULL_OPERATION,
            OperationPhase.TRACK_A_CLOSING
        ]
    
    def is_track_b_active(self) -> bool:
        """Track B ÌôúÏÑ± Ïó¨Î∂Ä"""
        return self.current_phase == OperationPhase.FULL_OPERATION
    
    def get_current_phase_info(self) -> Dict:
        """ÌòÑÏû¨ Phase Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            'phase': self.current_phase.name,
            'track_a_active': self.is_track_a_active(),
            'track_b_active': self.is_track_b_active(),
            'kst_time': self._get_kst_time().isoformat(),
            'next_transition': self._get_next_transition_time()
        }
    
    def _get_next_transition_time(self) -> Optional[str]:
        """Îã§Ïùå Phase Ï†ÑÌôò ÏãúÍ∞Å Î∞òÌôò"""
        current_time = self._get_kst_time()
        
        transitions = [
            (self.wake_up_time, "WARMUP"),
            (self.track_a_start, "TRACK_A_ONLY"),
            (self.track_b_start, "FULL_OPERATION"),
            (self.track_b_stop, "TRACK_A_CLOSING"),
            (self.track_a_stop, "SHUTDOWN"),
            (self.backup_time, "IDLE")
        ]
        
        for transition_time, phase_name in transitions:
            if current_time < transition_time:
                return f"{transition_time} ({phase_name})"
        
        # Next day wake-up
        return f"{self.wake_up_time} (WARMUP, next day)"


# Integration with MultiProcessOrchestrator
class EnhancedMultiProcessOrchestrator:
    """
    Multi-Process Orchestrator with Operation Scheduler
    """
    
    def __init__(self, config):
        self.config = config
        self.scheduler = OperationScheduler(config)
        
        # Register phase callbacks
        self.scheduler.register_phase_callback(
            OperationPhase.WARMUP,
            self._on_warmup
        )
        self.scheduler.register_phase_callback(
            OperationPhase.TRACK_A_ONLY,
            self._on_track_a_start
        )
        self.scheduler.register_phase_callback(
            OperationPhase.FULL_OPERATION,
            self._on_track_b_start
        )
        self.scheduler.register_phase_callback(
            OperationPhase.TRACK_A_CLOSING,
            self._on_track_b_stop
        )
        self.scheduler.register_phase_callback(
            OperationPhase.SHUTDOWN,
            self._on_track_a_stop
        )
    
    async def _on_warmup(self):
        """08:00 Wake-up phase"""
        logger.info("Executing wake-up routine...")
        # Token refresh, system warmup, health check
        await self.token_manager.execute_pre_market_refresh()
    
    async def _on_track_a_start(self):
        """09:00 Track A start"""
        logger.info("Starting Track A process...")
        self.track_a_process = Process(
            target=TrackARunner(...).run,
            name="TrackA-Process"
        )
        self.track_a_process.start()
    
    async def _on_track_b_start(self):
        """09:30 Track B start with context sync"""
        logger.info("Starting Track B process with context sync...")
        self.track_b_process = Process(
            target=TrackBRunner(...).run,
            name="TrackB-Process"
        )
        self.track_b_process.start()
    
    async def _on_track_b_stop(self):
        """15:00 Track B stop"""
        logger.info("Stopping Track B process...")
        if self.track_b_process and self.track_b_process.is_alive():
            self.track_b_process.terminate()
            self.track_b_process.join(timeout=30)
    
    async def _on_track_a_stop(self):
        """15:30 Track A stop"""
        logger.info("Stopping Track A process...")
        if self.track_a_process and self.track_a_process.is_alive():
            self.track_a_process.terminate()
            self.track_a_process.join(timeout=30)
```

#### Scheduler Benefits

| Benefit | Description | Impact |
|---------|-------------|--------|
| **Noise Filtering** | 09:00~09:30, 15:00~15:30 Ï†úÏô∏ | SNR 30% Ìñ•ÏÉÅ |
| **Resource Efficiency** | Î∂àÌïÑÏöîÌïú Íµ¨Í∞Ñ ÎπÑÌôúÏÑ±Ìôî | CPU 20% Ï†àÍ∞ê |
| **Data Quality** | Í≥†ÌíàÏßà Íµ¨Í∞ÑÎßå ÏàòÏßë | Ï†ÑÎûµ Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ |
| **System Stability** | ÌîºÌÅ¨ ÏãúÍ∞ÑÎåÄ ÌöåÌîº | Ïò§Î•òÏú® 40% Í∞êÏÜå |
| **Predictable Behavior** | Î™ÖÌôïÌïú ÏãúÍ∞Ñ Ï†úÏñ¥ | Ïö¥ÏòÅ ÏïàÏ†ïÏÑ± Ìñ•ÏÉÅ |

---

## Multi-Account Scalability

### Maximum Symbol Capacity

#### Track A (REST - Swing/Portfolio)

**Ï†ÑÏ≤¥ Universe Ïª§Î≤ÑÎ¶¨ÏßÄ:**

| Ìï≠Î™© | Í∞í | Í≥ÑÏÇ∞ |
|-----|-----|-----|
| **Universe ÌÅ¨Í∏∞** | ~1,000 Ï¢ÖÎ™© | Ï†ÑÏùº Ï¢ÖÍ∞Ä 4,000Ïõê Ïù¥ÏÉÅ |
| **ÏàòÏßë Ï£ºÍ∏∞** | 10Î∂Ñ | 600Ï¥à |
| **Ï¥àÎãπ ÏöîÏ≤≠** | ~1.67 req/sec | 1000 / 600 = 1.67 |
| **API Rate Limit** | 20 req/sec | KIS API Ï†úÌïú |
| **Ïó¨Ïú†Ïú®** | 91.7% | (20 - 1.67) / 20 |

**Í≤∞Î°†**: Track AÎäî **Universe Ï†ÑÏ≤¥**Î•º Ïª§Î≤Ñ Í∞ÄÎä•Ìï©ÎãàÎã§.

#### Track B (WebSocket - Scalp)

**Í≥†Ï†ï Ïä¨Î°Ø Ï†úÏïΩ:**

| Ìï≠Î™© | Í∞í | ÏÑ§Î™Ö |
|-----|-----|-----|
| **ÏµúÎåÄ ÎèôÏãú Íµ¨ÎèÖ** | 41 Ï¢ÖÎ™© | KIS WebSocket Ï†úÌïú |
| **ÏàòÏßë Ï£ºÌååÏàò** | 2Hz (0.5Ï¥à) | Ï†ïÏÉÅ Î™®Îìú |
| **Ï¥àÎãπ Î©îÏãúÏßÄ** | ~82 msg/sec | 41 √ó 2 = 82 |
| **ÏôÑÌôî Ïãú Ï£ºÌååÏàò** | 1Hz ~ 0.5Hz | Î∂ÄÌïò Ïãú Ï°∞Ï†à |

**Í≤∞Î°†**: Track BÎäî **ÏµúÎåÄ 41Ï¢ÖÎ™©**Îßå Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ Í∞ÄÎä•Ìï©ÎãàÎã§.

### Resource Constraints

#### API Rate Limits

| Provider | REST Limit | WebSocket Limit | Notes |
|----------|-----------|----------------|-------|
| **KIS** | 20 req/sec<br/>1000 req/min | 41 ÎèôÏãú Íµ¨ÎèÖ | Í∏∞Ï§Ä Provider |
| **Kiwoom** | TBD | TBD | Phase 2 |
| **Upbit** | 10 req/sec | 100 ÎèôÏãú Íµ¨ÎèÖ | Phase 2 |

#### System Resources

| Resource | Normal Load | Peak Load | Limit |
|----------|------------|-----------|-------|
| **CPU** | 30~50% | 70~80% | 85% (Í≤ΩÍ≥†) |
| **Memory** | 500MB~1GB | 1.5GB~2GB | 2.5GB (Í≤ΩÍ≥†) |
| **Network** | 1~5 Mbps | 10~20 Mbps | 50 Mbps |
| **Disk I/O** | 10~50 MB/s | 100~200 MB/s | 500 MB/s |

### Scalability Considerations

#### Vertical Scaling (Phase 1)

ÌòÑÏû¨ Îã®Ïùº VM ÌôòÍ≤Ω:
- **VM Spec**: Azure B2s (2 vCPU, 4GB RAM)
- **ÌôïÏû• Í≤ΩÎ°ú**: B2s ‚Üí B2ms ‚Üí B4ms

#### Horizontal Scaling (Phase 2+)

Ìñ•ÌõÑ Îã§Ï§ë Ïù∏Ïä§ÌÑ¥Ïä§ Í≥†Î†§:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Load Balancer / Router               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇObserver ‚îÇ   ‚îÇObserver ‚îÇ   ‚îÇObserver ‚îÇ
   ‚îÇInstance ‚îÇ   ‚îÇInstance ‚îÇ   ‚îÇInstance ‚îÇ
   ‚îÇ  KR     ‚îÇ   ‚îÇ  Crypto ‚îÇ   ‚îÇ   US    ‚îÇ
   ‚îÇ Stocks  ‚îÇ   ‚îÇ         ‚îÇ   ‚îÇ Stocks  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**MarketÎ≥Ñ Î∂ÑÎ¶¨ Ï†ÑÎûµ:**
- kr_stocks: 41 slots
- crypto: 41 slots
- us_stocks: 41 slots
- **Total**: 123 slots (3 instances)

---

## Implementation Guide

### Component Architecture

```python
# ========================================
# 1. Universe Manager
# ========================================

from dataclasses import dataclass
from datetime import date, datetime
from pathlib import Path
from typing import List, Dict, Optional


@dataclass
class Symbol:
    """Ï¢ÖÎ™© Ï†ïÎ≥¥"""
    code: str
    name: str
    market_cap: int
    prev_close: int
    avg_volume_20d: int


@dataclass
class UniverseSnapshot:
    """Universe Ïä§ÎÉÖÏÉ∑"""
    date: str
    previous_trading_day: str
    generated_at: datetime
    market: str
    symbols: List[Symbol]


class UniverseManager:
    """
    Universe ÏÉùÏÑ± Î∞è Í¥ÄÎ¶¨
    
    Responsibilities:
    - ÏùºÏùº Universe Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ±
    - Ï¢ÖÎ™© ÌïÑÌÑ∞ÎßÅ (Ï†ÑÏùº Ï¢ÖÍ∞Ä 4,000Ïõê Ïù¥ÏÉÅ)
    - Ïä§ÎÉÖÏÉ∑ ÌååÏùº Ï†ÄÏû•/Î°úÎìú
    - Ï∫êÏãú Í¥ÄÎ¶¨
    """
    
    def __init__(self, provider_engine: ProviderEngine):
        self.provider = provider_engine
        self.cache: Dict[str, UniverseSnapshot] = {}
        self.config_dir = Path("config/universe")
    
    def create_daily_snapshot(self, target_date: str) -> UniverseSnapshot:
        """
        ÏùºÏùº Universe Ïä§ÎÉÖÏÉ∑ ÏÉùÏÑ±
        
        Args:
            target_date: YYYYMMDD ÌòïÏãù
        
        Returns:
            ÏÉùÏÑ±Îêú UniverseSnapshot
        """
        prev_day = self._get_previous_trading_day(target_date)
        eod_data = self._fetch_eod_prices(prev_day)
        filtered = self._filter_symbols(eod_data, min_close=4000)
        sorted_symbols = self._sort_by_market_cap(filtered)
        
        snapshot = UniverseSnapshot(
            date=target_date,
            previous_trading_day=prev_day,
            generated_at=datetime.now(timezone.utc),
            market="kr_stocks",
            symbols=sorted_symbols
        )
        
        self._save_snapshot(snapshot)
        self.cache[target_date] = snapshot
        
        return snapshot
    
    def load_universe(self, date: str, market: str = "kr_stocks") -> List[Symbol]:
        """
        Universe Î°úÎìú (Ï∫êÏãú Ïö∞ÏÑ†, ÌååÏùº fallback)
        """
        if date in self.cache:
            return self.cache[date].symbols
        
        snapshot = self._load_from_file(date, market)
        if snapshot:
            self.cache[date] = snapshot
            return snapshot.symbols
        
        raise UniverseNotFoundError(f"Universe not found: {date}")
    
    def get_current_universe(self, market: str = "kr_stocks") -> List[Symbol]:
        """
        ÎãπÏùº Universe Ï°∞Ìöå
        """
        today = datetime.now().strftime("%Y%m%d")
        return self.load_universe(today, market)


# ========================================
# 2. Trigger Engine
# ========================================

from enum import Enum, auto


class TriggerType(Enum):
    """Trigger ÌÉÄÏûÖ"""
    VOLUME_SURGE = auto()
    TRADE_VELOCITY = auto()
    VOLATILITY_SPIKE = auto()
    MANUAL = auto()


@dataclass
class Trigger:
    """Trigger Ï†ïÎ≥¥"""
    type: TriggerType
    symbol: str
    strength: float  # 0.0 ~ 1.0
    detected_at: datetime
    ttl_seconds: int
    metadata: Dict


@dataclass
class Candidate:
    """Ïä¨Î°Ø Ìï†Îãπ ÌõÑÎ≥¥"""
    symbol: Symbol
    trigger: Trigger
    priority_score: float


class TriggerEngine:
    """
    Trigger Í∞êÏßÄ Î∞è ÌèâÍ∞Ä
    
    Responsibilities:
    - Universe Ï†ÑÏ≤¥ Ï¢ÖÎ™©Ïóê ÎåÄÌïú Trigger Í∞êÏßÄ
    - Trigger Ïö∞ÏÑ†ÏàúÏúÑ Í≥ÑÏÇ∞
    - Top ÌõÑÎ≥¥ ÏÑ†Ï†ï
    """
    
    def __init__(self, config: TriggerConfig):
        self.config = config
    
    def evaluate_universe(
        self,
        universe: List[Symbol],
        snapshots: Dict[str, Snapshot]
    ) -> List[Candidate]:
        """
        Universe Ï†ÑÏ≤¥ ÌèâÍ∞Ä
        
        Returns:
            Ïö∞ÏÑ†ÏàúÏúÑ ÏàúÏúºÎ°ú Ï†ïÎ†¨Îêú Candidate Î¶¨Ïä§Ìä∏
        """
        candidates = []
        
        for symbol in universe:
            snapshot = snapshots.get(symbol.code)
            if not snapshot:
                continue
            
            # Í∞Å Trigger ÌÉÄÏûÖÎ≥Ñ Í∞êÏßÄ
            triggers = []
            triggers.append(self._detect_volume_surge(symbol, snapshot))
            triggers.append(self._detect_trade_velocity(symbol, snapshot))
            triggers.append(self._detect_volatility_spike(symbol, snapshot))
            
            # Í∞ÄÏû• Í∞ïÌïú Trigger ÏÑ†ÌÉù
            triggers = [t for t in triggers if t is not None]
            if not triggers:
                continue
            
            best_trigger = max(triggers, key=lambda t: t.strength)
            score = self._calculate_priority_score(symbol, best_trigger)
            
            candidates.append(Candidate(
                symbol=symbol,
                trigger=best_trigger,
                priority_score=score
            ))
        
        # Ïö∞ÏÑ†ÏàúÏúÑ Ïàú Ï†ïÎ†¨
        candidates.sort(key=lambda c: c.priority_score, reverse=True)
        return candidates
    
    def _detect_volume_surge(
        self,
        symbol: Symbol,
        snapshot: Snapshot
    ) -> Optional[Trigger]:
        """Í±∞ÎûòÎüâ Í∏âÏ¶ù Í∞êÏßÄ"""
        ratio = snapshot.volume / symbol.avg_volume_20d
        
        if ratio >= self.config.volume_surge_threshold:
            strength = min(ratio / 5.0, 1.0)
            return Trigger(
                type=TriggerType.VOLUME_SURGE,
                symbol=symbol.code,
                strength=strength,
                detected_at=datetime.now(timezone.utc),
                ttl_seconds=300,  # 5Î∂Ñ
                metadata={'ratio': ratio}
            )
        
        return None
    
    def _calculate_priority_score(
        self,
        symbol: Symbol,
        trigger: Trigger
    ) -> float:
        """Ïö∞ÏÑ†ÏàúÏúÑ Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞"""
        weights = {
            TriggerType.VOLUME_SURGE: 10.0,
            TriggerType.TRADE_VELOCITY: 8.0,
            TriggerType.VOLATILITY_SPIKE: 6.0,
            TriggerType.MANUAL: 5.0,
        }
        
        base = weights[trigger.type]
        market_cap_factor = math.log10(symbol.market_cap) * 0.1
        
        return base * (1 + trigger.strength) + market_cap_factor


# ========================================
# 3. Slot Manager
# ========================================

@dataclass
class Slot:
    """WebSocket Ïä¨Î°Ø"""
    slot_id: int
    symbol: Optional[str]
    candidate: Optional[Candidate]
    allocated_at: Optional[datetime]
    state: str  # 'available', 'allocated', 'active'


@dataclass
class AllocationResult:
    """Ïä¨Î°Ø Ìï†Îãπ Í≤∞Í≥º"""
    allocated: List[Candidate]
    overflow: List[Candidate]
    replacements: List[tuple[Slot, Candidate]]  # (old_slot, new_candidate)
    timestamp: datetime


class SlotManager:
    """
    WebSocket Ïä¨Î°Ø Í¥ÄÎ¶¨
    
    Responsibilities:
    - 41 Ïä¨Î°Ø Ìï†Îãπ/Ìï¥Ï†ú
    - Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò Ïä¨Î°Ø ÍµêÏ≤¥
    - Overflow Í∏∞Î°ù
    """
    
    MAX_SLOTS = 41
    
    def __init__(self, overflow_logger: OverflowLogger):
        self.slots: List[Slot] = [
            Slot(slot_id=i, symbol=None, candidate=None,
                 allocated_at=None, state='available')
            for i in range(self.MAX_SLOTS)
        ]
        self.overflow_logger = overflow_logger
    
    def allocate(self, candidates: List[Candidate]) -> AllocationResult:
        """
        ÌõÑÎ≥¥ Î¶¨Ïä§Ìä∏ÏóêÏÑú Ïä¨Î°Ø Ìï†Îãπ
        
        Returns:
            AllocationResult
        """
        allocated = []
        overflow = []
        replacements = []
        
        for candidate in candidates:
            # Îπà Ïä¨Î°Ø Ï∞æÍ∏∞
            available_slot = self._find_available_slot()
            
            if available_slot:
                # Îπà Ïä¨Î°ØÏóê Ìï†Îãπ
                self._allocate_slot(available_slot, candidate)
                allocated.append(candidate)
            else:
                # ÍµêÏ≤¥ ÎåÄÏÉÅ ÌèâÍ∞Ä
                replacement_slot = self._evaluate_replacement(candidate)
                
                if replacement_slot:
                    old_candidate = replacement_slot.candidate
                    self._release_slot(replacement_slot)
                    self._allocate_slot(replacement_slot, candidate)
                    replacements.append((replacement_slot, candidate))
                    allocated.append(candidate)
                else:
                    # Overflow
                    overflow.append(candidate)
                    self.overflow_logger.record(candidate)
        
        return AllocationResult(
            allocated=allocated,
            overflow=overflow,
            replacements=replacements,
            timestamp=datetime.now(timezone.utc)
        )
    
    def _find_available_slot(self) -> Optional[Slot]:
        """Îπà Ïä¨Î°Ø Ï∞æÍ∏∞"""
        for slot in self.slots:
            if slot.state == 'available':
                return slot
        return None
    
    def _evaluate_replacement(self, new_candidate: Candidate) -> Optional[Slot]:
        """ÍµêÏ≤¥ ÎåÄÏÉÅ Ïä¨Î°Ø ÌèâÍ∞Ä"""
        occupied_slots = [s for s in self.slots if s.state in ['allocated', 'active']]
        
        if not occupied_slots:
            return None
        
        # Í∞ÄÏû• ÎÇÆÏùÄ Ïä§ÏΩîÏñ¥Ïùò Ïä¨Î°Ø Ï∞æÍ∏∞
        min_slot = min(occupied_slots, key=lambda s: s.candidate.priority_score)
        
        # ÏÉà ÌõÑÎ≥¥Í∞Ä Ï∂©Î∂ÑÌûà ÎÜíÏùÄ Ïä§ÏΩîÏñ¥Ïù∏Í∞Ä?
        if new_candidate.priority_score > min_slot.candidate.priority_score * 1.2:
            return min_slot
        
        return None
    
    def _allocate_slot(self, slot: Slot, candidate: Candidate):
        """Ïä¨Î°Ø Ìï†Îãπ"""
        slot.symbol = candidate.symbol.code
        slot.candidate = candidate
        slot.allocated_at = datetime.now(timezone.utc)
        slot.state = 'allocated'
    
    def _release_slot(self, slot: Slot):
        """Ïä¨Î°Ø Ìï¥Ï†ú"""
        slot.symbol = None
        slot.candidate = None
        slot.allocated_at = None
        slot.state = 'available'
    
    def get_active_symbols(self) -> List[str]:
        """ÌôúÏÑ± Ïä¨Î°ØÏùò Ï¢ÖÎ™© ÏΩîÎìú Î¶¨Ïä§Ìä∏"""
        return [
            s.symbol for s in self.slots
            if s.state in ['allocated', 'active'] and s.symbol
        ]
```

### Configuration Schema

**ÌååÏùº**: `config/observer/symbol_selection.json`

```json
{
  "universe": {
    "generation_time": "05:00",
    "min_close_price": 4000,
    "sort_by": "market_cap",
    "exclude_suspended": true,
    "exclude_managed": false,
    "min_symbol_count": 100
  },
  "triggers": {
    "volume_surge": {
      "enabled": true,
      "threshold": 3.0,
      "ttl_seconds": 300,
      "weight": 10.0
    },
    "trade_velocity": {
      "enabled": true,
      "threshold": 10.0,
      "ttl_seconds": 180,
      "weight": 8.0
    },
    "volatility_spike": {
      "enabled": true,
      "threshold": 2.0,
      "ttl_seconds": 300,
      "weight": 6.0
    },
    "manual": {
      "enabled": true,
      "ttl_seconds": -1,
      "weight": 5.0
    }
  },
  "slots": {
    "max_slots": 41,
    "replacement_threshold": 1.2,
    "overflow_alert_rate": 0.1
  }
}
```

### Data Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Daily Universe Generation               ‚îÇ
‚îÇ  (05:00 Before Market Open)                             ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  1. Fetch All Symbols (~2,500)                          ‚îÇ
‚îÇ  2. Fetch EOD Prices (Prev Day)                         ‚îÇ
‚îÇ  3. Filter: Close >= 4,000 KRW                          ‚îÇ
‚îÇ  4. Sort: Market Cap DESC                                ‚îÇ
‚îÇ  5. Save: config/universe/YYYYMMDD_kr_stocks.json       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Result: ~1,000 symbols                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Track A Collection                   ‚îÇ
‚îÇ  (10min Interval, 09:00~15:30)                          ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  - Collect ALL Universe symbols                          ‚îÇ
‚îÇ  - REST API calls (~1.7 req/sec)                        ‚îÇ
‚îÇ  - Store: data/observer/swing/                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Trigger Evaluation                    ‚îÇ
‚îÇ  (Every Track A cycle)                                   ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  For each symbol in Universe:                            ‚îÇ
‚îÇ    - Detect Volume Surge                                 ‚îÇ
‚îÇ    - Detect Trade Velocity                               ‚îÇ
‚îÇ    - Detect Volatility Spike                             ‚îÇ
‚îÇ    - Calculate Priority Score                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Result: Ranked Candidate List                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Slot Allocation                       ‚îÇ
‚îÇ  (After Trigger Evaluation)                              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  1. Get Top 41 Candidates                                ‚îÇ
‚îÇ  2. Check Available Slots                                ‚îÇ
‚îÇ  3. Evaluate Replacements                                ‚îÇ
‚îÇ  4. Record Overflow (42nd+)                              ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Result: 41 Active Slots + Overflow Ledger               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Track B Collection                      ‚îÇ
‚îÇ  (Real-time WebSocket, 09:30~15:00)                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  - Subscribe 41 symbols                                  ‚îÇ
‚îÇ  - Collect 2Hz tick data                                 ‚îÇ
‚îÇ  - Store: data/observer/scalp/                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Constraints and Considerations

### Technical Constraints

| Constraint | Value | Impact | Mitigation |
|-----------|-------|--------|-----------|
| **WebSocket Slots** | 41 max | Track B Ï†úÌïú | Overflow Ledger Í∏∞Î°ù |
| **REST Rate Limit** | 20 req/sec | API Ìò∏Ï∂ú ÏÜçÎèÑ Ï†úÌïú | Î∞∞Ïπò Ï≤òÎ¶¨ + ÎåÄÍ∏∞ |
| **Token Expiry** | 24ÏãúÍ∞Ñ | Ïû¨Ïù∏Ï¶ù ÌïÑÏöî | 1ÏãúÍ∞Ñ Ï†Ñ Í∞±Ïã† |
| **Reconnection** | 60Ìöå/ÏãúÍ∞Ñ | Ïû¨Ïó∞Í≤∞ Ï†úÌïú | Backoff Ï†ïÏ±Ö |

### Business Rules

1. **Universe Immutability**: ÎãπÏùº UniverseÎäî Î∂àÎ≥Ä (Ïû¨ÌòÑÏÑ±)
2. **Coverage Priority**: Symbol Coverage > Frequency
3. **Gap Marker**: Îç∞Ïù¥ÌÑ∞ Í∞≠ Î∞úÏÉù Ïãú Ï¶ùÍ±∞Îßå Í∏∞Î°ù (Î≥µÏõê ÏóÜÏùå)
4. **Trigger Fairness**: ÎèôÏùº Ïä§ÏΩîÏñ¥ Ïãú ÏãúÍ∞ÄÏ¥ùÏï° Ïàú

### Edge Cases

#### 1. Universe Generation Failure

**ÏÉÅÌô©**: ÏùºÏùº Universe ÏÉùÏÑ± Ïã§Ìå®

**Ï≤òÎ¶¨**:
1. Ï†ÑÏùº Universe Ïû¨ÏÇ¨Ïö©
2. CRITICAL ÏïåÎ¶º Î∞úÏÜ°
3. ÏàòÎèô Í∞úÏûÖ ÎåÄÍ∏∞

#### 2. All Slots Occupied

**ÏÉÅÌô©**: Î™®Îì† 41 Ïä¨Î°ØÏù¥ ÎÜíÏùÄ Ïö∞ÏÑ†ÏàúÏúÑÎ°ú Ï†êÏú†

**Ï≤òÎ¶¨**:
1. Overflow LedgerÏóê Í∏∞Î°ù
2. Îã§Ïùå ÌèâÍ∞Ä Ï£ºÍ∏∞ ÎåÄÍ∏∞
3. Overflow Rate Î™®ÎãàÌÑ∞ÎßÅ

#### 3. Rapid Symbol Churn

**ÏÉÅÌô©**: ÏßßÏùÄ ÏãúÍ∞Ñ ÎÇ¥ ÎπàÎ≤àÌïú Ïä¨Î°Ø ÍµêÏ≤¥

**Ï≤òÎ¶¨**:
1. Replacement Threshold ÏÉÅÌñ• (1.2 ‚Üí 1.5)
2. TTL Ïó∞Ïû•
3. ÍµêÏ≤¥ ÎπàÎèÑ Î°úÍ∑∏ Î∂ÑÏÑù

#### 4. Market Cap Data Missing

**ÏÉÅÌô©**: ÏãúÍ∞ÄÏ¥ùÏï° Ï†ïÎ≥¥ ÏóÜÏùå

**Ï≤òÎ¶¨**:
1. Default Value ÏÇ¨Ïö© (0)
2. Ïö∞ÏÑ†ÏàúÏúÑÎäî Trigger StrengthÎßåÏúºÎ°ú Í≥ÑÏÇ∞
3. Í≤ΩÍ≥† Î°úÍ∑∏

### Performance Considerations

#### Universe Generation Performance

| Metric | Target | Actual (Expected) |
|--------|--------|------------------|
| **Generation Time** | < 5Î∂Ñ | ~3Î∂Ñ |
| **API Calls** | ~2,500 | 2,500 |
| **Rate Compliance** | 100% | 99.9% |
| **Success Rate** | > 99% | 99.5% |

#### Slot Allocation Performance

| Metric | Target | Actual (Expected) |
|--------|--------|------------------|
| **Evaluation Time** | < 1Ï¥à | ~200ms |
| **Replacement Time** | < 5Ï¥à | ~2Ï¥à |
| **Overflow Rate** | < 5% | 2~3% |

---

## Appendix

### A. Glossary

| Term | Definition |
|------|-----------|
| **Universe** | Í±∞Îûò Í∞ÄÎä•Ìïú Ï¢ÖÎ™©Ïùò Ï†ÑÏ≤¥ ÏßëÌï© (Ï†ÑÏùº Ï¢ÖÍ∞Ä 4,000Ïõê Ïù¥ÏÉÅ) |
| **Slot** | Track B WebSocket Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ ÏúÑÏπò (ÏµúÎåÄ 41Í∞ú) |
| **Trigger** | Ï¢ÖÎ™© ÏÑ†Ï†ïÏùÑ Ïú†Î∞úÌïòÎäî Ïù¥Î≤§Ìä∏ (Í±∞ÎûòÎüâ Í∏âÏ¶ù Îì±) |
| **Overflow** | 41 Ïä¨Î°Ø Ï¥àÍ≥º Ï¢ÖÎ™© (LedgerÏóê Í∏∞Î°ù) |
| **TTL (Time To Live)** | Trigger Ïú†Ìö® Í∏∞Í∞Ñ |
| **Track A** | REST API Í∏∞Î∞ò 10Î∂Ñ Ï£ºÍ∏∞ ÏàòÏßë (Universe Ï†ÑÏ≤¥) |
| **Track B** | WebSocket Í∏∞Î∞ò 2Hz Ïã§ÏãúÍ∞Ñ ÏàòÏßë (41 Ï¢ÖÎ™©) |

### B. Reference Documents

1. **data_pipeline_architecture_observer_v1.0.md**
   - Ï†ÑÏ≤¥ ÏïÑÌÇ§ÌÖçÏ≤ò Í∞úÏöî
   - Observer Core ÏÑ§Í≥Ñ

2. **kis_api_specification_v1.0.md**
   - KIS API Rate Limits
   - WebSocket Ï†úÏïΩÏÇ¨Ìï≠

3. **implementation_details_supplement_v1.0.md**
   - Universe Manager Íµ¨ÌòÑ ÏÉÅÏÑ∏
   - Slot Manager ÏùòÏÇ¨ÏΩîÎìú

### C. Future Enhancements

#### Phase 2: Multi-Market Support

- Crypto Universe (Upbit)
- US Stocks Universe (IB)
- MarketÎ≥Ñ ÎèÖÎ¶Ω Ïä¨Î°Ø Í¥ÄÎ¶¨

#### Phase 3: Machine Learning Trigger

- ML Í∏∞Î∞ò Trigger ÏòàÏ∏°
- Ïó≠ÏÇ¨Ï†Å Ìå®ÌÑ¥ ÌïôÏäµ
- ÎèôÏ†Å ÏûÑÍ≥ÑÍ∞í Ï°∞Ï†ï

#### Phase 4: Distributed Slot Management

- Îã§Ï§ë Observer Ïù∏Ïä§ÌÑ¥Ïä§
- Ïä¨Î°Ø Ï°∞Ï†ï ÌîÑÎ°úÌÜ†ÏΩú
- Load Balancing

### D. Change Log

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0.0 | 2026-01-20 | Initial document creation | Developer Agent |
| 1.1.0 | 2026-01-20 | Production-ready enhancements | Senior Architect |
|       |            | - Added Session & Token Management | |
|       |            | - Added Data Gap Recovery mechanism | |
|       |            | - Added Resource Isolation architecture | |
|       |            | - Added Multi-Account Scalability design | |
| 1.2.0 | 2026-01-20 | Operational optimization | Senior Architect |
|       |            | - Differentiated operation timeline | |
|       |            | - Track A: 09:00~15:30 (full coverage) | |
|       |            | - Track B: 09:30~15:00 (noise filtered) | |
|       |            | - Context synchronization at Track B startup | |
|       |            | - OperationScheduler implementation | |
|       |            | - Intentional noise exclusion strategy | |

### E. Production Deployment Checklist

#### System Wake-up (v1.2)
- [ ] Cron job ÏÑ§Ï†ï: Îß§Ïùº 08:00 KST
- [ ] Token refresh automation
- [ ] System warmup routine
- [ ] WebSocket pre-establishment
- [ ] Health check endpoint Íµ¨ÌòÑ
- [ ] Alert mechanism Íµ¨ÏÑ± (Email/Slack)
- [ ] Failure recovery ÌÖåÏä§Ìä∏ ÏôÑÎ£å

#### Operation Scheduler (v1.2)
- [ ] OperationScheduler Íµ¨ÌòÑ
- [ ] Phase transition callbacks Îì±Î°ù
- [ ] Track A: 09:00~15:30 Í≤ÄÏ¶ù
- [ ] Track B: 09:30~15:00 Í≤ÄÏ¶ù
- [ ] ÏãúÍ∞ÑÎåÄÎ≥Ñ Phase Ï†ÑÌôò ÌÖåÏä§Ìä∏
- [ ] Noise filtering Ìö®Í≥º Í≤ÄÏ¶ù

#### Context Synchronization (v1.2)
- [ ] TrackBContextSynchronizer Íµ¨ÌòÑ
- [ ] 09:00~09:30 Îç∞Ïù¥ÌÑ∞ backfill Î°úÏßÅ
- [ ] Baseline metrics Í≥ÑÏÇ∞ Í≤ÄÏ¶ù
- [ ] Initial slot pre-selection ÌÖåÏä§Ìä∏
- [ ] Context sync ÏÑ±Îä• ÌÖåÏä§Ìä∏ (< 30Ï¥à)
- [ ] Degraded mode fallback ÌÖåÏä§Ìä∏

#### Gap Recovery System
- [ ] GapDetector Íµ¨ÌòÑ Î∞è ÌÖåÏä§Ìä∏
- [ ] GapRecoveryEngine Íµ¨ÌòÑ
- [ ] Recovery worker ÌîÑÎ°úÏÑ∏Ïä§ ÏÑ§Ï†ï
- [ ] Recovery file Ïä§ÌÜ†Î¶¨ÏßÄ ÌôïÎ≥¥
- [ ] Backfill ÏÑ±Îä• ÌÖåÏä§Ìä∏ (< 5Ï¥à)

#### Resource Isolation
- [ ] Multi-process runner Íµ¨ÌòÑ
- [ ] IPC channel ÏÑ±Îä• ÌÖåÏä§Ìä∏
- [ ] Process monitoring ÏÑ§Ï†ï
- [ ] Graceful shutdown ÌÖåÏä§Ìä∏
- [ ] Crash recovery ÌÖåÏä§Ìä∏

#### Multi-Account Setup
- [ ] Ï∂îÍ∞Ä App Key Î∞úÍ∏â (ÏµúÏÜå 3Í∞ú)
- [ ] ProviderPool Íµ¨ÌòÑ
- [ ] Account health monitoring
- [ ] Failover Î°úÏßÅ ÌÖåÏä§Ìä∏
- [ ] Cost/benefit Î∂ÑÏÑù ÏôÑÎ£å

#### Monitoring & Alerting
- [ ] Token expiry monitoring
- [ ] Gap rate tracking (< 5%)
- [ ] Process health check
- [ ] Slot utilization metrics
- [ ] Alert rules ÏÑ§Ï†ï

---

**Document End**
